agents:
- agent_type: conversation
  llm:
    llm_type: gpt-3.5-turbo
    model: gpt-3.5-turbo
    model_type: gpt-3.5-turbo
    temperature: 0.0
  memory:
    memory_type: judge
  name: security_researcher
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - code_author
  - moderator
  - review_board
  role_description: "You are the Security Researcher. Identify all potential security\
    \ vulnerabilities in the given <code> snippet. \nProvide your output as a JSON\
    \ array. Each element in the array represents one identified vulnerability and\
    \ should include:\n- `vulnerability`: A short name or description of the vulnerability.\n\
    - `reason`: A detailed explanation of why this is a vulnerability and how it could\
    \ be exploited.\n- `impact`: The potential consequences if this vulnerability\
    \ were exploited.\n\nNow please analyze the following code.\n\n<code>:\n  void\
    \ Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t,\
    \ *shape_t, *dense_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"sp_indices\", &indices_t));\n\
    \    OP_REQUIRES_OK(ctx, ctx->input(\"sp_values\", &values_t));\n    OP_REQUIRES_OK(ctx,\
    \ ctx->input(\"sp_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"\
    dense\", &dense_t));\n\n    // Validations.\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices_t->shape()),\n\
    \                errors::InvalidArgument(\n                    \"Input sp_indices\
    \ should be a matrix but received shape: \",\n                    indices_t->shape().DebugString()));\n\
    \    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(values_t->shape())\
    \ &&\n                    TensorShapeUtils::IsVector(shape_t->shape()),\n    \
    \            errors::InvalidArgument(\n                    \"Inputs sp_values\
    \ and sp_shape should be vectors \"\n                    \"but received shapes:\
    \ \",\n                    values_t->shape().DebugString(), \" and \",\n     \
    \               shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx,\
    \ TensorShapeUtils::IsVector(shape_t->shape()),\n        errors::InvalidArgument(\"\
    Input sp_shape must be a vector. Got: \",\n                                shape_t->shape().DebugString()));\n\
    \    OP_REQUIRES(\n        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n\
    \        errors::InvalidArgument(\n            \"The first dimension of values\
    \ and indices should match. (\",\n            values_t->dim_size(0), \" vs. \"\
    , indices_t->dim_size(0), \")\"));\n    OP_REQUIRES(\n        ctx, shape_t->shape().dim_size(0)\
    \ == indices_t->shape().dim_size(1),\n        errors::InvalidArgument(\n     \
    \       \"Number of dimensions must match second dimension of indices. \",\n \
    \           \"Got \", shape_t->shape().dim_size(0),\n            \" dimensions,\
    \ indices shape: \", indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\
    \ shape_t->NumElements() > 0,\n                errors::InvalidArgument(\n    \
    \                \"The shape argument requires at least one element.\"));\n\n\
    \    const auto indices_mat = indices_t->matrix<int64_t>();\n    const auto shape_vec\
    \ = shape_t->vec<int64_t>();\n    const auto lhs_dims = BCast::FromShape(TensorShape(shape_vec));\n\
    \    const auto rhs_dims = BCast::FromShape(dense_t->shape());\n    BCast b(lhs_dims,\
    \ rhs_dims, false);  // false for keeping the same num dims.\n\n    // True iff\
    \ (size(lhs) >= size(rhs)) and all dims in lhs is greater or equal\n    // to\
    \ dims in rhs (from right to left).\n    auto VecGreaterEq = [](ArraySlice<int64_t>\
    \ lhs, ArraySlice<int64_t> rhs) {\n      if (lhs.size() < rhs.size()) return false;\n\
    \      for (size_t i = 0; i < rhs.size(); ++i) {\n        if (lhs[lhs.size() -\
    \ 1 - i] < rhs[rhs.size() - 1 - i]) return false;\n      }\n      return true;\n\
    \    };\n    OP_REQUIRES(ctx, VecGreaterEq(lhs_dims, rhs_dims) && b.IsValid(),\n\
    \                errors::InvalidArgument(\n                    \"SparseDenseBinaryOpShared\
    \ broadcasts dense to sparse \"\n                    \"only; got incompatible\
    \ shapes: [\",\n                    absl::StrJoin(lhs_dims, \",\"), \"] vs. [\"\
    ,\n                    absl::StrJoin(rhs_dims, \",\"), \"]\"));\n\n    Tensor\
    \ *output_values = nullptr;\n    Tensor dense_gathered;\n    const int64_t nnz\
    \ = indices_t->dim_size(0);\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0,\
    \ TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\
    \ TensorShape({nnz}),\n                                &dense_gathered));\n  \
    \  bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(),\
    \ \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from\
    \ the dense side, with reshape and broadcasting\n    // *of the dense side* taken\
    \ into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n\
    \    // We can directly use the sparse indices to look up dense side, because\n\
    \    // \"b.y_reshape()\" and \"b.y_bcast()\" are guaranteed to have rank \"ndims\"\
    .\n    auto dense_gathered_flat = dense_gathered.flat<T>();\n    const int ndims\
    \ = lhs_dims.size();\n    switch (ndims) {\n#define CASE(NDIM)               \
    \                                              \\\n  case NDIM: {            \
    \                                                     \\\n    TensorRef<Eigen::Tensor<const\
    \ T, NDIM, Eigen::RowMajor>> rhs_ref =         \\\n        dense_t->shaped<T,\
    \ NDIM>(b.y_reshape())                                \\\n            .broadcast(BCast::ToIndexArray<NDIM>(b.y_bcast()));\
    \                \\\n    Eigen::array<Eigen::DenseIndex, NDIM> idx;          \
    \                       \\\n    bool indices_valid = true;                   \
    \                              \\\n    for (int i = 0; i < nnz; ++i) {       \
    \                                     \\\n      for (int d = 0; d < NDIM; ++d)\
    \ {                                         \\\n        idx[d] = internal::SubtleMustCopy(indices_mat(i,\
    \ d));                  \\\n        if (!FastBoundsCheck(idx[d], rhs_ref.dimension(d)))\
    \ {                  \\\n          indices_valid = false;                    \
    \                           \\\n        }                                    \
    \                                  \\\n      }                               \
    \                                         \\\n      OP_REQUIRES(             \
    \                                                \\\n          ctx, indices_valid,\
    \                                                  \\\n          errors::InvalidArgument(\"\
    Provided indices are out-of-bounds w.r.t. \" \\\n                            \
    \      \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i)\
    \ = rhs_ref.coeff(idx);                             \\\n      if (op_is_div) {\
    \                                                         \\\n        OP_REQUIRES(ctx,\
    \ dense_gathered_flat(i) != 0,                          \\\n                 \
    \   errors::InvalidArgument(                                   \\\n          \
    \              \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n \
    \                       \"but input dense tensor contains zero \"));         \
    \    \\\n      }                                                             \
    \           \\\n    }                                                        \
    \                  \\\n    break;                                            \
    \                         \\\n  }\n\n      CASE(1);\n      CASE(2);\n      CASE(3);\n\
    \      CASE(4);\n      CASE(5);\n      default:\n        OP_REQUIRES(\n      \
    \      ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks\
    \ between 1 and 5 \"\n                                    \"are currently supported.\
    \  Tensor rank: \",\n                                    ndims));\n#undef CASE\n\
    \    }\n\n    output_values->flat<T>().device(ctx->eigen_device<Device>()) =\n\
    \        values_t->flat<T>().binaryExpr(dense_gathered_flat,\n               \
    \                        typename Functor::func());\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-3.5-turbo
    model: gpt-3.5-turbo
    model_type: gpt-3.5-turbo
    temperature: 0.0
  memory:
    memory_type: judge
  name: code_author
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - security_researcher
  - moderator
  - review_board
  role_description: "You are the Code Author of <code>. The Security Researcher has\
    \ presented a JSON array of alleged vulnerabilities. \nYou must respond as if\
    \ you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\xE2\u20AC\u2122s entry.\n- `response_type`: 'refutation' if you believe\
    \ this concern is unfounded, or 'mitigation' if you acknowledge it and propose\
    \ a workable solution.\n- `reason`: A concise explanation of why the vulnerability\
    \ is refuted or how you propose to mitigate it.\n\n<code>:\n  void Compute(OpKernelContext\
    \ *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *dense_t;\n\
    \    OP_REQUIRES_OK(ctx, ctx->input(\"sp_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx,\
    \ ctx->input(\"sp_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"\
    sp_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense\", &dense_t));\n\
    \n    // Validations.\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices_t->shape()),\n\
    \                errors::InvalidArgument(\n                    \"Input sp_indices\
    \ should be a matrix but received shape: \",\n                    indices_t->shape().DebugString()));\n\
    \    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(values_t->shape())\
    \ &&\n                    TensorShapeUtils::IsVector(shape_t->shape()),\n    \
    \            errors::InvalidArgument(\n                    \"Inputs sp_values\
    \ and sp_shape should be vectors \"\n                    \"but received shapes:\
    \ \",\n                    values_t->shape().DebugString(), \" and \",\n     \
    \               shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx,\
    \ TensorShapeUtils::IsVector(shape_t->shape()),\n        errors::InvalidArgument(\"\
    Input sp_shape must be a vector. Got: \",\n                                shape_t->shape().DebugString()));\n\
    \    OP_REQUIRES(\n        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n\
    \        errors::InvalidArgument(\n            \"The first dimension of values\
    \ and indices should match. (\",\n            values_t->dim_size(0), \" vs. \"\
    , indices_t->dim_size(0), \")\"));\n    OP_REQUIRES(\n        ctx, shape_t->shape().dim_size(0)\
    \ == indices_t->shape().dim_size(1),\n        errors::InvalidArgument(\n     \
    \       \"Number of dimensions must match second dimension of indices. \",\n \
    \           \"Got \", shape_t->shape().dim_size(0),\n            \" dimensions,\
    \ indices shape: \", indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\
    \ shape_t->NumElements() > 0,\n                errors::InvalidArgument(\n    \
    \                \"The shape argument requires at least one element.\"));\n\n\
    \    const auto indices_mat = indices_t->matrix<int64_t>();\n    const auto shape_vec\
    \ = shape_t->vec<int64_t>();\n    const auto lhs_dims = BCast::FromShape(TensorShape(shape_vec));\n\
    \    const auto rhs_dims = BCast::FromShape(dense_t->shape());\n    BCast b(lhs_dims,\
    \ rhs_dims, false);  // false for keeping the same num dims.\n\n    // True iff\
    \ (size(lhs) >= size(rhs)) and all dims in lhs is greater or equal\n    // to\
    \ dims in rhs (from right to left).\n    auto VecGreaterEq = [](ArraySlice<int64_t>\
    \ lhs, ArraySlice<int64_t> rhs) {\n      if (lhs.size() < rhs.size()) return false;\n\
    \      for (size_t i = 0; i < rhs.size(); ++i) {\n        if (lhs[lhs.size() -\
    \ 1 - i] < rhs[rhs.size() - 1 - i]) return false;\n      }\n      return true;\n\
    \    };\n    OP_REQUIRES(ctx, VecGreaterEq(lhs_dims, rhs_dims) && b.IsValid(),\n\
    \                errors::InvalidArgument(\n                    \"SparseDenseBinaryOpShared\
    \ broadcasts dense to sparse \"\n                    \"only; got incompatible\
    \ shapes: [\",\n                    absl::StrJoin(lhs_dims, \",\"), \"] vs. [\"\
    ,\n                    absl::StrJoin(rhs_dims, \",\"), \"]\"));\n\n    Tensor\
    \ *output_values = nullptr;\n    Tensor dense_gathered;\n    const int64_t nnz\
    \ = indices_t->dim_size(0);\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0,\
    \ TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\
    \ TensorShape({nnz}),\n                                &dense_gathered));\n  \
    \  bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(),\
    \ \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from\
    \ the dense side, with reshape and broadcasting\n    // *of the dense side* taken\
    \ into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n\
    \    // We can directly use the sparse indices to look up dense side, because\n\
    \    // \"b.y_reshape()\" and \"b.y_bcast()\" are guaranteed to have rank \"ndims\"\
    .\n    auto dense_gathered_flat = dense_gathered.flat<T>();\n    const int ndims\
    \ = lhs_dims.size();\n    switch (ndims) {\n#define CASE(NDIM)               \
    \                                              \\\n  case NDIM: {            \
    \                                                     \\\n    TensorRef<Eigen::Tensor<const\
    \ T, NDIM, Eigen::RowMajor>> rhs_ref =         \\\n        dense_t->shaped<T,\
    \ NDIM>(b.y_reshape())                                \\\n            .broadcast(BCast::ToIndexArray<NDIM>(b.y_bcast()));\
    \                \\\n    Eigen::array<Eigen::DenseIndex, NDIM> idx;          \
    \                       \\\n    bool indices_valid = true;                   \
    \                              \\\n    for (int i = 0; i < nnz; ++i) {       \
    \                                     \\\n      for (int d = 0; d < NDIM; ++d)\
    \ {                                         \\\n        idx[d] = internal::SubtleMustCopy(indices_mat(i,\
    \ d));                  \\\n        if (!FastBoundsCheck(idx[d], rhs_ref.dimension(d)))\
    \ {                  \\\n          indices_valid = false;                    \
    \                           \\\n        }                                    \
    \                                  \\\n      }                               \
    \                                         \\\n      OP_REQUIRES(             \
    \                                                \\\n          ctx, indices_valid,\
    \                                                  \\\n          errors::InvalidArgument(\"\
    Provided indices are out-of-bounds w.r.t. \" \\\n                            \
    \      \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i)\
    \ = rhs_ref.coeff(idx);                             \\\n      if (op_is_div) {\
    \                                                         \\\n        OP_REQUIRES(ctx,\
    \ dense_gathered_flat(i) != 0,                          \\\n                 \
    \   errors::InvalidArgument(                                   \\\n          \
    \              \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n \
    \                       \"but input dense tensor contains zero \"));         \
    \    \\\n      }                                                             \
    \           \\\n    }                                                        \
    \                  \\\n    break;                                            \
    \                         \\\n  }\n\n      CASE(1);\n      CASE(2);\n      CASE(3);\n\
    \      CASE(4);\n      CASE(5);\n      default:\n        OP_REQUIRES(\n      \
    \      ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks\
    \ between 1 and 5 \"\n                                    \"are currently supported.\
    \  Tensor rank: \",\n                                    ndims));\n#undef CASE\n\
    \    }\n\n    output_values->flat<T>().device(ctx->eigen_device<Device>()) =\n\
    \        values_t->flat<T>().binaryExpr(dense_gathered_flat,\n               \
    \                        typename Functor::func());\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-3.5-turbo
    model: gpt-3.5-turbo
    model_type: gpt-3.5-turbo
    temperature: 0.0
  memory:
    memory_type: judge
  name: moderator
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - review_board
  - security_researcher
  - code_author
  role_description: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\xE2\u20AC\u2122s identified\
    \ vulnerabilities and the Code Author\xE2\u20AC\u2122s responses, \nprovide a\
    \ single JSON object with two fields:\n- `researcher_summary`: A concise summary\
    \ of the vulnerabilities and reasoning presented by the Security Researcher.\n\
    - `author_summary`: A concise summary of the Code Author\xE2\u20AC\u2122s counterarguments\
    \ or mitigation strategies.\n\n<code>:\n  void Compute(OpKernelContext *ctx) override\
    \ {\n    const Tensor *indices_t, *values_t, *shape_t, *dense_t;\n    OP_REQUIRES_OK(ctx,\
    \ ctx->input(\"sp_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"\
    sp_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"sp_shape\", &shape_t));\n\
    \    OP_REQUIRES_OK(ctx, ctx->input(\"dense\", &dense_t));\n\n    // Validations.\n\
    \    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices_t->shape()),\n      \
    \          errors::InvalidArgument(\n                    \"Input sp_indices should\
    \ be a matrix but received shape: \",\n                    indices_t->shape().DebugString()));\n\
    \    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(values_t->shape())\
    \ &&\n                    TensorShapeUtils::IsVector(shape_t->shape()),\n    \
    \            errors::InvalidArgument(\n                    \"Inputs sp_values\
    \ and sp_shape should be vectors \"\n                    \"but received shapes:\
    \ \",\n                    values_t->shape().DebugString(), \" and \",\n     \
    \               shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx,\
    \ TensorShapeUtils::IsVector(shape_t->shape()),\n        errors::InvalidArgument(\"\
    Input sp_shape must be a vector. Got: \",\n                                shape_t->shape().DebugString()));\n\
    \    OP_REQUIRES(\n        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n\
    \        errors::InvalidArgument(\n            \"The first dimension of values\
    \ and indices should match. (\",\n            values_t->dim_size(0), \" vs. \"\
    , indices_t->dim_size(0), \")\"));\n    OP_REQUIRES(\n        ctx, shape_t->shape().dim_size(0)\
    \ == indices_t->shape().dim_size(1),\n        errors::InvalidArgument(\n     \
    \       \"Number of dimensions must match second dimension of indices. \",\n \
    \           \"Got \", shape_t->shape().dim_size(0),\n            \" dimensions,\
    \ indices shape: \", indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\
    \ shape_t->NumElements() > 0,\n                errors::InvalidArgument(\n    \
    \                \"The shape argument requires at least one element.\"));\n\n\
    \    const auto indices_mat = indices_t->matrix<int64_t>();\n    const auto shape_vec\
    \ = shape_t->vec<int64_t>();\n    const auto lhs_dims = BCast::FromShape(TensorShape(shape_vec));\n\
    \    const auto rhs_dims = BCast::FromShape(dense_t->shape());\n    BCast b(lhs_dims,\
    \ rhs_dims, false);  // false for keeping the same num dims.\n\n    // True iff\
    \ (size(lhs) >= size(rhs)) and all dims in lhs is greater or equal\n    // to\
    \ dims in rhs (from right to left).\n    auto VecGreaterEq = [](ArraySlice<int64_t>\
    \ lhs, ArraySlice<int64_t> rhs) {\n      if (lhs.size() < rhs.size()) return false;\n\
    \      for (size_t i = 0; i < rhs.size(); ++i) {\n        if (lhs[lhs.size() -\
    \ 1 - i] < rhs[rhs.size() - 1 - i]) return false;\n      }\n      return true;\n\
    \    };\n    OP_REQUIRES(ctx, VecGreaterEq(lhs_dims, rhs_dims) && b.IsValid(),\n\
    \                errors::InvalidArgument(\n                    \"SparseDenseBinaryOpShared\
    \ broadcasts dense to sparse \"\n                    \"only; got incompatible\
    \ shapes: [\",\n                    absl::StrJoin(lhs_dims, \",\"), \"] vs. [\"\
    ,\n                    absl::StrJoin(rhs_dims, \",\"), \"]\"));\n\n    Tensor\
    \ *output_values = nullptr;\n    Tensor dense_gathered;\n    const int64_t nnz\
    \ = indices_t->dim_size(0);\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0,\
    \ TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\
    \ TensorShape({nnz}),\n                                &dense_gathered));\n  \
    \  bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(),\
    \ \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from\
    \ the dense side, with reshape and broadcasting\n    // *of the dense side* taken\
    \ into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n\
    \    // We can directly use the sparse indices to look up dense side, because\n\
    \    // \"b.y_reshape()\" and \"b.y_bcast()\" are guaranteed to have rank \"ndims\"\
    .\n    auto dense_gathered_flat = dense_gathered.flat<T>();\n    const int ndims\
    \ = lhs_dims.size();\n    switch (ndims) {\n#define CASE(NDIM)               \
    \                                              \\\n  case NDIM: {            \
    \                                                     \\\n    TensorRef<Eigen::Tensor<const\
    \ T, NDIM, Eigen::RowMajor>> rhs_ref =         \\\n        dense_t->shaped<T,\
    \ NDIM>(b.y_reshape())                                \\\n            .broadcast(BCast::ToIndexArray<NDIM>(b.y_bcast()));\
    \                \\\n    Eigen::array<Eigen::DenseIndex, NDIM> idx;          \
    \                       \\\n    bool indices_valid = true;                   \
    \                              \\\n    for (int i = 0; i < nnz; ++i) {       \
    \                                     \\\n      for (int d = 0; d < NDIM; ++d)\
    \ {                                         \\\n        idx[d] = internal::SubtleMustCopy(indices_mat(i,\
    \ d));                  \\\n        if (!FastBoundsCheck(idx[d], rhs_ref.dimension(d)))\
    \ {                  \\\n          indices_valid = false;                    \
    \                           \\\n        }                                    \
    \                                  \\\n      }                               \
    \                                         \\\n      OP_REQUIRES(             \
    \                                                \\\n          ctx, indices_valid,\
    \                                                  \\\n          errors::InvalidArgument(\"\
    Provided indices are out-of-bounds w.r.t. \" \\\n                            \
    \      \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i)\
    \ = rhs_ref.coeff(idx);                             \\\n      if (op_is_div) {\
    \                                                         \\\n        OP_REQUIRES(ctx,\
    \ dense_gathered_flat(i) != 0,                          \\\n                 \
    \   errors::InvalidArgument(                                   \\\n          \
    \              \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n \
    \                       \"but input dense tensor contains zero \"));         \
    \    \\\n      }                                                             \
    \           \\\n    }                                                        \
    \                  \\\n    break;                                            \
    \                         \\\n  }\n\n      CASE(1);\n      CASE(2);\n      CASE(3);\n\
    \      CASE(4);\n      CASE(5);\n      default:\n        OP_REQUIRES(\n      \
    \      ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks\
    \ between 1 and 5 \"\n                                    \"are currently supported.\
    \  Tensor rank: \",\n                                    ndims));\n#undef CASE\n\
    \    }\n\n    output_values->flat<T>().device(ctx->eigen_device<Device>()) =\n\
    \        values_t->flat<T>().binaryExpr(dense_gathered_flat,\n               \
    \                        typename Functor::func());\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-3.5-turbo
    model: gpt-3.5-turbo
    model_type: gpt-3.5-turbo
    temperature: 0.0
  memory:
    memory_type: judge
  name: review_board
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver: []
  role_description: "You are the Review Board. After reviewing the Moderator\xE2\u20AC\
    \u2122s summary and <code> (if needed, the original arguments), \nproduce a JSON\
    \ array of verdicts for each vulnerability identified by the Security Researcher.\
    \ Each object in the array should include:\n- `vulnerability`: The same name as\
    \ given by the Security Researcher.\n- `decision`: One of 'valid', 'invalid',\
    \ or 'partially valid'.\n- `severity`: If valid or partially valid, assign a severity\
    \ ('low', 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`:\
    \ Suggest what should be done next (e.g., 'fix immediately', 'monitor', 'no action\
    \ needed').\n- `justification`: A brief explanation of why you reached this conclusion,\
    \ considering both the Security Researcher\xE2\u20AC\u2122s and Code Author\xE2\
    \u20AC\u2122s perspectives.\n\nYou need to analyze the code and evaluate the reasoning\
    \ provided by the Security Researcher, Code Author, and Moderator. Do not automatically\
    \ mark a decision as 'valid' just because the Code Author refutes it, nor mark\
    \ it as 'invalid' because the Security Researcher claims a vulnerability exists.\
    \ Instead, carefully assess whether their reasoning aligns with the actual security\
    \ implications and technical reality.\n\n<code>:\n  void Compute(OpKernelContext\
    \ *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *dense_t;\n\
    \    OP_REQUIRES_OK(ctx, ctx->input(\"sp_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx,\
    \ ctx->input(\"sp_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"\
    sp_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense\", &dense_t));\n\
    \n    // Validations.\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices_t->shape()),\n\
    \                errors::InvalidArgument(\n                    \"Input sp_indices\
    \ should be a matrix but received shape: \",\n                    indices_t->shape().DebugString()));\n\
    \    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(values_t->shape())\
    \ &&\n                    TensorShapeUtils::IsVector(shape_t->shape()),\n    \
    \            errors::InvalidArgument(\n                    \"Inputs sp_values\
    \ and sp_shape should be vectors \"\n                    \"but received shapes:\
    \ \",\n                    values_t->shape().DebugString(), \" and \",\n     \
    \               shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx,\
    \ TensorShapeUtils::IsVector(shape_t->shape()),\n        errors::InvalidArgument(\"\
    Input sp_shape must be a vector. Got: \",\n                                shape_t->shape().DebugString()));\n\
    \    OP_REQUIRES(\n        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n\
    \        errors::InvalidArgument(\n            \"The first dimension of values\
    \ and indices should match. (\",\n            values_t->dim_size(0), \" vs. \"\
    , indices_t->dim_size(0), \")\"));\n    OP_REQUIRES(\n        ctx, shape_t->shape().dim_size(0)\
    \ == indices_t->shape().dim_size(1),\n        errors::InvalidArgument(\n     \
    \       \"Number of dimensions must match second dimension of indices. \",\n \
    \           \"Got \", shape_t->shape().dim_size(0),\n            \" dimensions,\
    \ indices shape: \", indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\
    \ shape_t->NumElements() > 0,\n                errors::InvalidArgument(\n    \
    \                \"The shape argument requires at least one element.\"));\n\n\
    \    const auto indices_mat = indices_t->matrix<int64_t>();\n    const auto shape_vec\
    \ = shape_t->vec<int64_t>();\n    const auto lhs_dims = BCast::FromShape(TensorShape(shape_vec));\n\
    \    const auto rhs_dims = BCast::FromShape(dense_t->shape());\n    BCast b(lhs_dims,\
    \ rhs_dims, false);  // false for keeping the same num dims.\n\n    // True iff\
    \ (size(lhs) >= size(rhs)) and all dims in lhs is greater or equal\n    // to\
    \ dims in rhs (from right to left).\n    auto VecGreaterEq = [](ArraySlice<int64_t>\
    \ lhs, ArraySlice<int64_t> rhs) {\n      if (lhs.size() < rhs.size()) return false;\n\
    \      for (size_t i = 0; i < rhs.size(); ++i) {\n        if (lhs[lhs.size() -\
    \ 1 - i] < rhs[rhs.size() - 1 - i]) return false;\n      }\n      return true;\n\
    \    };\n    OP_REQUIRES(ctx, VecGreaterEq(lhs_dims, rhs_dims) && b.IsValid(),\n\
    \                errors::InvalidArgument(\n                    \"SparseDenseBinaryOpShared\
    \ broadcasts dense to sparse \"\n                    \"only; got incompatible\
    \ shapes: [\",\n                    absl::StrJoin(lhs_dims, \",\"), \"] vs. [\"\
    ,\n                    absl::StrJoin(rhs_dims, \",\"), \"]\"));\n\n    Tensor\
    \ *output_values = nullptr;\n    Tensor dense_gathered;\n    const int64_t nnz\
    \ = indices_t->dim_size(0);\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0,\
    \ TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\
    \ TensorShape({nnz}),\n                                &dense_gathered));\n  \
    \  bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(),\
    \ \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from\
    \ the dense side, with reshape and broadcasting\n    // *of the dense side* taken\
    \ into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n\
    \    // We can directly use the sparse indices to look up dense side, because\n\
    \    // \"b.y_reshape()\" and \"b.y_bcast()\" are guaranteed to have rank \"ndims\"\
    .\n    auto dense_gathered_flat = dense_gathered.flat<T>();\n    const int ndims\
    \ = lhs_dims.size();\n    switch (ndims) {\n#define CASE(NDIM)               \
    \                                              \\\n  case NDIM: {            \
    \                                                     \\\n    TensorRef<Eigen::Tensor<const\
    \ T, NDIM, Eigen::RowMajor>> rhs_ref =         \\\n        dense_t->shaped<T,\
    \ NDIM>(b.y_reshape())                                \\\n            .broadcast(BCast::ToIndexArray<NDIM>(b.y_bcast()));\
    \                \\\n    Eigen::array<Eigen::DenseIndex, NDIM> idx;          \
    \                       \\\n    bool indices_valid = true;                   \
    \                              \\\n    for (int i = 0; i < nnz; ++i) {       \
    \                                     \\\n      for (int d = 0; d < NDIM; ++d)\
    \ {                                         \\\n        idx[d] = internal::SubtleMustCopy(indices_mat(i,\
    \ d));                  \\\n        if (!FastBoundsCheck(idx[d], rhs_ref.dimension(d)))\
    \ {                  \\\n          indices_valid = false;                    \
    \                           \\\n        }                                    \
    \                                  \\\n      }                               \
    \                                         \\\n      OP_REQUIRES(             \
    \                                                \\\n          ctx, indices_valid,\
    \                                                  \\\n          errors::InvalidArgument(\"\
    Provided indices are out-of-bounds w.r.t. \" \\\n                            \
    \      \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i)\
    \ = rhs_ref.coeff(idx);                             \\\n      if (op_is_div) {\
    \                                                         \\\n        OP_REQUIRES(ctx,\
    \ dense_gathered_flat(i) != 0,                          \\\n                 \
    \   errors::InvalidArgument(                                   \\\n          \
    \              \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n \
    \                       \"but input dense tensor contains zero \"));         \
    \    \\\n      }                                                             \
    \           \\\n    }                                                        \
    \                  \\\n    break;                                            \
    \                         \\\n  }\n\n      CASE(1);\n      CASE(2);\n      CASE(3);\n\
    \      CASE(4);\n      CASE(5);\n      default:\n        OP_REQUIRES(\n      \
    \      ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks\
    \ between 1 and 5 \"\n                                    \"are currently supported.\
    \  Tensor rank: \",\n                                    ndims));\n#undef CASE\n\
    \    }\n\n    output_values->flat<T>().device(ctx->eigen_device<Device>()) =\n\
    \        values_t->flat<T>().binaryExpr(dense_gathered_flat,\n               \
    \                        typename Functor::func());\n  }"
  verbose: true
environment:
  env_type: judge
  id_save: 195340
  max_turns: 4
  rule:
    describer:
      type: basic
    order:
      type: judge
    selector:
      type: basic
    updater:
      type: basic
    visibility:
      type: all
  target: 1
  task_name: code_vulnerability_review
  unit_tests: None
prompts:
  code_author_role_prompt: "You are the Code Author of <code>. The Security Researcher\
    \ has presented a JSON array of alleged vulnerabilities. \nYou must respond as\
    \ if you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\xE2\u20AC\u2122s entry.\n- `response_type`: 'refutation' if you believe\
    \ this concern is unfounded, or 'mitigation' if you acknowledge it and propose\
    \ a workable solution.\n- `reason`: A concise explanation of why the vulnerability\
    \ is refuted or how you propose to mitigate it."
  moderator_role_prompt: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\xE2\u20AC\u2122s identified\
    \ vulnerabilities and the Code Author\xE2\u20AC\u2122s responses, \nprovide a\
    \ single JSON object with two fields:\n- `researcher_summary`: A concise summary\
    \ of the vulnerabilities and reasoning presented by the Security Researcher.\n\
    - `author_summary`: A concise summary of the Code Author\xE2\u20AC\u2122s counterarguments\
    \ or mitigation strategies."
  prompt: 'You are working in a programming team to check whether a code have a potential
    vulnerability in it.


    ${role_description}


    ${chat_history}'
  review_board_role_prompt: "You are the Review Board. After reviewing the Moderator\xE2\
    \u20AC\u2122s summary and <code> (if needed, the original arguments), \nproduce\
    \ a JSON array of verdicts for each vulnerability identified by the Security Researcher.\
    \ Each object in the array should include:\n- `vulnerability`: The same name as\
    \ given by the Security Researcher.\n- `decision`: One of 'valid', 'invalid',\
    \ or 'partially valid'.\n- `severity`: If valid or partially valid, assign a severity\
    \ ('low', 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`:\
    \ Suggest what should be done next (e.g., 'fix immediately', 'monitor', 'no action\
    \ needed').\n- `justification`: A brief explanation of why you reached this conclusion,\
    \ considering both the Security Researcher\xE2\u20AC\u2122s and Code Author\xE2\
    \u20AC\u2122s perspectives.\n\nYou need to analyze the code and evaluate the reasoning\
    \ provided by the Security Researcher, Code Author, and Moderator. Do not automatically\
    \ mark a decision as 'valid' just because the Code Author refutes it, nor mark\
    \ it as 'invalid' because the Security Researcher claims a vulnerability exists.\
    \ Instead, carefully assess whether their reasoning aligns with the actual security\
    \ implications and technical reality."
  security_researcher_role_prompt: "You are the Security Researcher. Identify all\
    \ potential security vulnerabilities in the given <code> snippet. \nProvide your\
    \ output as a JSON array. Each element in the array represents one identified\
    \ vulnerability and should include:\n- `vulnerability`: A short name or description\
    \ of the vulnerability.\n- `reason`: A detailed explanation of why this is a vulnerability\
    \ and how it could be exploited.\n- `impact`: The potential consequences if this\
    \ vulnerability were exploited.\n\nNow please analyze the following code."
