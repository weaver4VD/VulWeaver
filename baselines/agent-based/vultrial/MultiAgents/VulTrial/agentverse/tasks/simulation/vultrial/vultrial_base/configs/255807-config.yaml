agents:
- agent_type: conversation
  llm:
    llm_type: gpt-4o
    model: gpt-4o
    model_type: gpt-4o
    temperature: 0.0
  memory:
    memory_type: judge
  name: security_researcher
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - code_author
  - moderator
  - review_board
  role_description: "You are the Security Researcher. Identify all potential security\
    \ vulnerabilities in the given <code> snippet. \nProvide your output as a JSON\
    \ array. Each element in the array represents one identified vulnerability and\
    \ should include:\n- `vulnerability`: A short name or description of the vulnerability.\n\
    - `reason`: A detailed explanation of why this is a vulnerability and how it could\
    \ be exploited.\n- `impact`: The potential consequences if this vulnerability\
    \ were exploited.\n\nNow please analyze the following code.\n\n<code>:\nTfLiteStatus\
    \ Prepare(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params =\
    \ reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n  OpData* op_data\
    \ = reinterpret_cast<OpData*>(node->user_data);\n  int scratch_tensor_index =\
    \ op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs\
    \ we need.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TF_LITE_ENSURE_EQ(context,\
    \ node->inputs->size, 5);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n\
    \  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n\
    \                                          &weights_feature));\n  const TfLiteTensor*\
    \ weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node,\
    \ kWeightsTimeTensor, &weights_time));\n\n  TF_LITE_ENSURE(context,\n        \
    \         input->type == kTfLiteFloat32 || input->type == kTfLiteInt8);\n\n  //\
    \ Check all the parameters of tensor match within themselves and match the\n \
    \ // input configuration.\n  const int rank = params->rank;\n  const int batch_size\
    \ = input->dims->data[0];\n  const int num_filters = weights_feature->dims->data[0];\n\
    \  TF_LITE_ENSURE(context, rank != 0);\n  TF_LITE_ENSURE_EQ(context, num_filters\
    \ % rank, 0);\n  const int num_units = num_filters / rank;\n  const int memory_size\
    \ = weights_time->dims->data[1];\n  TF_LITE_ENSURE_EQ(context, input->dims->data[1],\n\
    \                    weights_feature->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context,\
    \ weights_time->dims->data[0], num_filters);\n\n  const TfLiteTensor* bias = GetOptionalInputTensor(context,\
    \ node, kBiasTensor);\n  if (bias) {\n    TF_LITE_ENSURE_EQ(context, bias->dims->data[0],\
    \ num_units);\n  }\n\n  const TfLiteTensor* state;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kStateTensor, &state));\n  TfLiteTensor* output;\n\
    \  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node,\
    \ kOutputTensor, &output));\n\n  // Check the shape of input state tensors.\n\
    \  TF_LITE_ENSURE_EQ(context, NumDimensions(state), 2);\n  TF_LITE_ENSURE_EQ(context,\
    \ SizeOfDimension(state, 0), batch_size);\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(state,\
    \ 1),\n                    memory_size * num_filters);\n\n  // Resize output.\n\
    \  TfLiteIntArray* output_size_array = TfLiteIntArrayCreate(2);\n  output_size_array->data[0]\
    \ = batch_size;\n  output_size_array->data[1] = num_units;\n  TF_LITE_ENSURE_OK(context,\n\
    \                    context->ResizeTensor(context, output, output_size_array));\n\
    \n  // The weights are of consistent type, so it suffices to check one.\n  const\
    \ bool is_hybrid_op = IsHybridOp(input, weights_feature);\n  const bool is_full_integer\
    \ = input->type == kTfLiteInt8;\n\n  // Resize scratch.\n  TfLiteIntArrayFree(node->temporaries);\n\
    \  if (is_hybrid_op) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  }\
    \ else if (is_full_integer) {\n    node->temporaries = TfLiteIntArrayCreate(2);\n\
    \  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[0]\
    \ = scratch_tensor_index;\n\n  TfLiteIntArray* scratch_size_array = TfLiteIntArrayCreate(2);\n\
    \  scratch_size_array->data[0] = batch_size;\n  scratch_size_array->data[1] =\
    \ num_filters;\n\n  TfLiteTensor* scratch_tensor;\n  TF_LITE_ENSURE_OK(\n    \
    \  context, GetTemporarySafe(context, node, /*index=*/0, &scratch_tensor));\n\n\
    \  // The scratch buffer is of type int32 for full integer svdf and it's of type\n\
    \  // float32 for hybrid and float case.\n  if (is_full_integer) {\n    scratch_tensor->type\
    \ = kTfLiteInt32;\n  } else {\n    scratch_tensor->type = kTfLiteFloat32;\n  }\n\
    \  scratch_tensor->allocation_type = kTfLiteArenaRw;\n  TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scratch_tensor,\n                           \
    \                        scratch_size_array));\n\n  if (is_hybrid_op) {\n    op_data->compute_row_sums\
    \ = true;\n    // Tell interpreter to allocate temporary tensors to store quantized\
    \ values\n    // of input tensors.\n    node->temporaries->data[1] = scratch_tensor_index\
    \ + 1;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context,\
    \ node, /*index=*/1,\n                                                &input_quantized));\n\
    \    input_quantized->type = weights_feature->type;\n    input_quantized->allocation_type\
    \ = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims))\
    \ {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n\
    \      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n\
    \                                                       input_quantized_size));\n\
    \    }\n\n    // Tell interpreter to allocate temporary tensors to store scaling\
    \ factors.\n    node->temporaries->data[2] = scratch_tensor_index + 2;\n    TfLiteTensor*\
    \ scaling_factors;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node,\
    \ /*index=*/2,\n                                                &scaling_factors));\n\
    \    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type\
    \ = kTfLiteArenaRw;\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims,\
    \ 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n\
    \      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scaling_factors,\n                          \
    \                             scaling_factors_size));\n    }\n\n    // Used to\
    \ store dequantized weights_time matrix for hybrid computation of\n    // matmul(state,\
    \ weights_time), which occurs in floating point.\n    node->temporaries->data[3]\
    \ = scratch_tensor_index + 3;\n    TfLiteTensor* float_weights_time;\n    TF_LITE_ENSURE_OK(context,\
    \ GetTemporarySafe(context, node, /*index=*/3,\n                             \
    \                   &float_weights_time));\n    float_weights_time->type = kTfLiteFloat32;\n\
    \    // Persistent so that we can compute the dequantized weights only once.\n\
    \    float_weights_time->allocation_type = kTfLiteArenaRwPersistent;\n    if (!TfLiteIntArrayEqual(float_weights_time->dims,\
    \ weights_time->dims)) {\n      TfLiteIntArray* float_weights_time_size =\n  \
    \        TfLiteIntArrayCopy(weights_time->dims);\n      TF_LITE_ENSURE_OK(context,\n\
    \                        context->ResizeTensor(context, float_weights_time,\n\
    \                                              float_weights_time_size));\n  \
    \  }\n\n    node->temporaries->data[4] = scratch_tensor_index + 4;\n    TfLiteTensor*\
    \ zero_points;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context,\
    \ node, /*index=*/4, &zero_points));\n    zero_points->type = kTfLiteFloat32;\n\
    \    zero_points->allocation_type = kTfLiteArenaRw;\n    int zero_points_dims[1]\
    \ = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(zero_points->dims, 1, zero_points_dims))\
    \ {\n      TfLiteIntArray* zero_points_size = TfLiteIntArrayCreate(1);\n     \
    \ zero_points_size->data[0] = zero_points_dims[0];\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, zero_points,\n                              \
    \                         zero_points_size));\n    }\n\n    node->temporaries->data[5]\
    \ = scratch_tensor_index + 5;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n\
    \                      GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n\
    \    row_sums->type = kTfLiteFloat32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n\
    \    int row_sums_dims[1] = {num_filters};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims,\
    \ 1, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n\
    \      row_sums_size->data[0] = row_sums_dims[0];\n      TF_LITE_ENSURE_OK(\n\
    \          context, context->ResizeTensor(context, row_sums, row_sums_size));\n\
    \    }\n  }\n  if (is_full_integer) {\n    // Allocated one extra tensor.\n  \
    \  TfLiteIntArray* output_temp_size_array = TfLiteIntArrayCreate(2);\n    output_temp_size_array->data[0]\
    \ = num_units;\n    output_temp_size_array->data[1] = batch_size;\n    node->temporaries->data[1]\
    \ = scratch_tensor_index + 1;\n    TfLiteTensor* output_temp;\n    TF_LITE_ENSURE_OK(\n\
    \        context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\
    \    output_temp->type = kTfLiteInt32;\n    output_temp->allocation_type = kTfLiteArenaRw;\n\
    \    TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_temp,\n\
    \                                                     output_temp_size_array));\n\
    \n    // Calculate effective scales.\n    TF_LITE_ENSURE(context, input->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* input_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_feature->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_feature->quantization.params);\n    TF_LITE_ENSURE(context, state->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* state_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_time->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_time->quantization.params);\n    TF_LITE_ENSURE(context, output->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        output->quantization.params);\n    const double effective_scale_1 = input_params->scale->data[0]\
    \ *\n                                     weights_feature_params->scale->data[0]\
    \ /\n                                     state_params->scale->data[0];\n    const\
    \ double effective_scale_2 = state_params->scale->data[0] *\n                \
    \                     weight_time_params->scale->data[0] /\n                 \
    \                    output_params->scale->data[0];\n    QuantizeMultiplier(effective_scale_1,\
    \ &op_data->effective_scale_1_a,\n                       &op_data->effective_scale_1_b);\n\
    \    QuantizeMultiplier(effective_scale_2, &op_data->effective_scale_2_a,\n  \
    \                     &op_data->effective_scale_2_b);\n  }\n  return kTfLiteOk;\n\
    }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-4o
    model: gpt-4o
    model_type: gpt-4o
    temperature: 0.0
  memory:
    memory_type: judge
  name: code_author
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - security_researcher
  - moderator
  - review_board
  role_description: "You are the Code Author of <code>. The Security Researcher has\
    \ presented a JSON array of alleged vulnerabilities. \nYou must respond as if\
    \ you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\xE2\u20AC\u2122s entry.\n- `response_type`: 'refutation' if you believe\
    \ this concern is unfounded, or 'mitigation' if you acknowledge it and propose\
    \ a workable solution.\n- `reason`: A concise explanation of why the vulnerability\
    \ is refuted or how you propose to mitigate it.\n\n<code>:\nTfLiteStatus Prepare(TfLiteContext*\
    \ context, TfLiteNode* node) {\n  const auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n\
    \  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int scratch_tensor_index\
    \ = op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs\
    \ we need.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TF_LITE_ENSURE_EQ(context,\
    \ node->inputs->size, 5);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n\
    \  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n\
    \                                          &weights_feature));\n  const TfLiteTensor*\
    \ weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node,\
    \ kWeightsTimeTensor, &weights_time));\n\n  TF_LITE_ENSURE(context,\n        \
    \         input->type == kTfLiteFloat32 || input->type == kTfLiteInt8);\n\n  //\
    \ Check all the parameters of tensor match within themselves and match the\n \
    \ // input configuration.\n  const int rank = params->rank;\n  const int batch_size\
    \ = input->dims->data[0];\n  const int num_filters = weights_feature->dims->data[0];\n\
    \  TF_LITE_ENSURE(context, rank != 0);\n  TF_LITE_ENSURE_EQ(context, num_filters\
    \ % rank, 0);\n  const int num_units = num_filters / rank;\n  const int memory_size\
    \ = weights_time->dims->data[1];\n  TF_LITE_ENSURE_EQ(context, input->dims->data[1],\n\
    \                    weights_feature->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context,\
    \ weights_time->dims->data[0], num_filters);\n\n  const TfLiteTensor* bias = GetOptionalInputTensor(context,\
    \ node, kBiasTensor);\n  if (bias) {\n    TF_LITE_ENSURE_EQ(context, bias->dims->data[0],\
    \ num_units);\n  }\n\n  const TfLiteTensor* state;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kStateTensor, &state));\n  TfLiteTensor* output;\n\
    \  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node,\
    \ kOutputTensor, &output));\n\n  // Check the shape of input state tensors.\n\
    \  TF_LITE_ENSURE_EQ(context, NumDimensions(state), 2);\n  TF_LITE_ENSURE_EQ(context,\
    \ SizeOfDimension(state, 0), batch_size);\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(state,\
    \ 1),\n                    memory_size * num_filters);\n\n  // Resize output.\n\
    \  TfLiteIntArray* output_size_array = TfLiteIntArrayCreate(2);\n  output_size_array->data[0]\
    \ = batch_size;\n  output_size_array->data[1] = num_units;\n  TF_LITE_ENSURE_OK(context,\n\
    \                    context->ResizeTensor(context, output, output_size_array));\n\
    \n  // The weights are of consistent type, so it suffices to check one.\n  const\
    \ bool is_hybrid_op = IsHybridOp(input, weights_feature);\n  const bool is_full_integer\
    \ = input->type == kTfLiteInt8;\n\n  // Resize scratch.\n  TfLiteIntArrayFree(node->temporaries);\n\
    \  if (is_hybrid_op) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  }\
    \ else if (is_full_integer) {\n    node->temporaries = TfLiteIntArrayCreate(2);\n\
    \  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[0]\
    \ = scratch_tensor_index;\n\n  TfLiteIntArray* scratch_size_array = TfLiteIntArrayCreate(2);\n\
    \  scratch_size_array->data[0] = batch_size;\n  scratch_size_array->data[1] =\
    \ num_filters;\n\n  TfLiteTensor* scratch_tensor;\n  TF_LITE_ENSURE_OK(\n    \
    \  context, GetTemporarySafe(context, node, /*index=*/0, &scratch_tensor));\n\n\
    \  // The scratch buffer is of type int32 for full integer svdf and it's of type\n\
    \  // float32 for hybrid and float case.\n  if (is_full_integer) {\n    scratch_tensor->type\
    \ = kTfLiteInt32;\n  } else {\n    scratch_tensor->type = kTfLiteFloat32;\n  }\n\
    \  scratch_tensor->allocation_type = kTfLiteArenaRw;\n  TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scratch_tensor,\n                           \
    \                        scratch_size_array));\n\n  if (is_hybrid_op) {\n    op_data->compute_row_sums\
    \ = true;\n    // Tell interpreter to allocate temporary tensors to store quantized\
    \ values\n    // of input tensors.\n    node->temporaries->data[1] = scratch_tensor_index\
    \ + 1;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context,\
    \ node, /*index=*/1,\n                                                &input_quantized));\n\
    \    input_quantized->type = weights_feature->type;\n    input_quantized->allocation_type\
    \ = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims))\
    \ {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n\
    \      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n\
    \                                                       input_quantized_size));\n\
    \    }\n\n    // Tell interpreter to allocate temporary tensors to store scaling\
    \ factors.\n    node->temporaries->data[2] = scratch_tensor_index + 2;\n    TfLiteTensor*\
    \ scaling_factors;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node,\
    \ /*index=*/2,\n                                                &scaling_factors));\n\
    \    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type\
    \ = kTfLiteArenaRw;\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims,\
    \ 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n\
    \      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scaling_factors,\n                          \
    \                             scaling_factors_size));\n    }\n\n    // Used to\
    \ store dequantized weights_time matrix for hybrid computation of\n    // matmul(state,\
    \ weights_time), which occurs in floating point.\n    node->temporaries->data[3]\
    \ = scratch_tensor_index + 3;\n    TfLiteTensor* float_weights_time;\n    TF_LITE_ENSURE_OK(context,\
    \ GetTemporarySafe(context, node, /*index=*/3,\n                             \
    \                   &float_weights_time));\n    float_weights_time->type = kTfLiteFloat32;\n\
    \    // Persistent so that we can compute the dequantized weights only once.\n\
    \    float_weights_time->allocation_type = kTfLiteArenaRwPersistent;\n    if (!TfLiteIntArrayEqual(float_weights_time->dims,\
    \ weights_time->dims)) {\n      TfLiteIntArray* float_weights_time_size =\n  \
    \        TfLiteIntArrayCopy(weights_time->dims);\n      TF_LITE_ENSURE_OK(context,\n\
    \                        context->ResizeTensor(context, float_weights_time,\n\
    \                                              float_weights_time_size));\n  \
    \  }\n\n    node->temporaries->data[4] = scratch_tensor_index + 4;\n    TfLiteTensor*\
    \ zero_points;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context,\
    \ node, /*index=*/4, &zero_points));\n    zero_points->type = kTfLiteFloat32;\n\
    \    zero_points->allocation_type = kTfLiteArenaRw;\n    int zero_points_dims[1]\
    \ = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(zero_points->dims, 1, zero_points_dims))\
    \ {\n      TfLiteIntArray* zero_points_size = TfLiteIntArrayCreate(1);\n     \
    \ zero_points_size->data[0] = zero_points_dims[0];\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, zero_points,\n                              \
    \                         zero_points_size));\n    }\n\n    node->temporaries->data[5]\
    \ = scratch_tensor_index + 5;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n\
    \                      GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n\
    \    row_sums->type = kTfLiteFloat32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n\
    \    int row_sums_dims[1] = {num_filters};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims,\
    \ 1, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n\
    \      row_sums_size->data[0] = row_sums_dims[0];\n      TF_LITE_ENSURE_OK(\n\
    \          context, context->ResizeTensor(context, row_sums, row_sums_size));\n\
    \    }\n  }\n  if (is_full_integer) {\n    // Allocated one extra tensor.\n  \
    \  TfLiteIntArray* output_temp_size_array = TfLiteIntArrayCreate(2);\n    output_temp_size_array->data[0]\
    \ = num_units;\n    output_temp_size_array->data[1] = batch_size;\n    node->temporaries->data[1]\
    \ = scratch_tensor_index + 1;\n    TfLiteTensor* output_temp;\n    TF_LITE_ENSURE_OK(\n\
    \        context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\
    \    output_temp->type = kTfLiteInt32;\n    output_temp->allocation_type = kTfLiteArenaRw;\n\
    \    TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_temp,\n\
    \                                                     output_temp_size_array));\n\
    \n    // Calculate effective scales.\n    TF_LITE_ENSURE(context, input->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* input_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_feature->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_feature->quantization.params);\n    TF_LITE_ENSURE(context, state->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* state_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_time->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_time->quantization.params);\n    TF_LITE_ENSURE(context, output->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        output->quantization.params);\n    const double effective_scale_1 = input_params->scale->data[0]\
    \ *\n                                     weights_feature_params->scale->data[0]\
    \ /\n                                     state_params->scale->data[0];\n    const\
    \ double effective_scale_2 = state_params->scale->data[0] *\n                \
    \                     weight_time_params->scale->data[0] /\n                 \
    \                    output_params->scale->data[0];\n    QuantizeMultiplier(effective_scale_1,\
    \ &op_data->effective_scale_1_a,\n                       &op_data->effective_scale_1_b);\n\
    \    QuantizeMultiplier(effective_scale_2, &op_data->effective_scale_2_a,\n  \
    \                     &op_data->effective_scale_2_b);\n  }\n  return kTfLiteOk;\n\
    }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-4o
    model: gpt-4o
    model_type: gpt-4o
    temperature: 0.0
  memory:
    memory_type: judge
  name: moderator
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - review_board
  - security_researcher
  - code_author
  role_description: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\xE2\u20AC\u2122s identified\
    \ vulnerabilities and the Code Author\xE2\u20AC\u2122s responses, \nprovide a\
    \ single JSON object with two fields:\n- `researcher_summary`: A concise summary\
    \ of the vulnerabilities and reasoning presented by the Security Researcher.\n\
    - `author_summary`: A concise summary of the Code Author\xE2\u20AC\u2122s counterarguments\
    \ or mitigation strategies.\n\n<code>:\nTfLiteStatus Prepare(TfLiteContext* context,\
    \ TfLiteNode* node) {\n  const auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n\
    \  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int scratch_tensor_index\
    \ = op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs\
    \ we need.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TF_LITE_ENSURE_EQ(context,\
    \ node->inputs->size, 5);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n\
    \  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n\
    \                                          &weights_feature));\n  const TfLiteTensor*\
    \ weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node,\
    \ kWeightsTimeTensor, &weights_time));\n\n  TF_LITE_ENSURE(context,\n        \
    \         input->type == kTfLiteFloat32 || input->type == kTfLiteInt8);\n\n  //\
    \ Check all the parameters of tensor match within themselves and match the\n \
    \ // input configuration.\n  const int rank = params->rank;\n  const int batch_size\
    \ = input->dims->data[0];\n  const int num_filters = weights_feature->dims->data[0];\n\
    \  TF_LITE_ENSURE(context, rank != 0);\n  TF_LITE_ENSURE_EQ(context, num_filters\
    \ % rank, 0);\n  const int num_units = num_filters / rank;\n  const int memory_size\
    \ = weights_time->dims->data[1];\n  TF_LITE_ENSURE_EQ(context, input->dims->data[1],\n\
    \                    weights_feature->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context,\
    \ weights_time->dims->data[0], num_filters);\n\n  const TfLiteTensor* bias = GetOptionalInputTensor(context,\
    \ node, kBiasTensor);\n  if (bias) {\n    TF_LITE_ENSURE_EQ(context, bias->dims->data[0],\
    \ num_units);\n  }\n\n  const TfLiteTensor* state;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kStateTensor, &state));\n  TfLiteTensor* output;\n\
    \  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node,\
    \ kOutputTensor, &output));\n\n  // Check the shape of input state tensors.\n\
    \  TF_LITE_ENSURE_EQ(context, NumDimensions(state), 2);\n  TF_LITE_ENSURE_EQ(context,\
    \ SizeOfDimension(state, 0), batch_size);\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(state,\
    \ 1),\n                    memory_size * num_filters);\n\n  // Resize output.\n\
    \  TfLiteIntArray* output_size_array = TfLiteIntArrayCreate(2);\n  output_size_array->data[0]\
    \ = batch_size;\n  output_size_array->data[1] = num_units;\n  TF_LITE_ENSURE_OK(context,\n\
    \                    context->ResizeTensor(context, output, output_size_array));\n\
    \n  // The weights are of consistent type, so it suffices to check one.\n  const\
    \ bool is_hybrid_op = IsHybridOp(input, weights_feature);\n  const bool is_full_integer\
    \ = input->type == kTfLiteInt8;\n\n  // Resize scratch.\n  TfLiteIntArrayFree(node->temporaries);\n\
    \  if (is_hybrid_op) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  }\
    \ else if (is_full_integer) {\n    node->temporaries = TfLiteIntArrayCreate(2);\n\
    \  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[0]\
    \ = scratch_tensor_index;\n\n  TfLiteIntArray* scratch_size_array = TfLiteIntArrayCreate(2);\n\
    \  scratch_size_array->data[0] = batch_size;\n  scratch_size_array->data[1] =\
    \ num_filters;\n\n  TfLiteTensor* scratch_tensor;\n  TF_LITE_ENSURE_OK(\n    \
    \  context, GetTemporarySafe(context, node, /*index=*/0, &scratch_tensor));\n\n\
    \  // The scratch buffer is of type int32 for full integer svdf and it's of type\n\
    \  // float32 for hybrid and float case.\n  if (is_full_integer) {\n    scratch_tensor->type\
    \ = kTfLiteInt32;\n  } else {\n    scratch_tensor->type = kTfLiteFloat32;\n  }\n\
    \  scratch_tensor->allocation_type = kTfLiteArenaRw;\n  TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scratch_tensor,\n                           \
    \                        scratch_size_array));\n\n  if (is_hybrid_op) {\n    op_data->compute_row_sums\
    \ = true;\n    // Tell interpreter to allocate temporary tensors to store quantized\
    \ values\n    // of input tensors.\n    node->temporaries->data[1] = scratch_tensor_index\
    \ + 1;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context,\
    \ node, /*index=*/1,\n                                                &input_quantized));\n\
    \    input_quantized->type = weights_feature->type;\n    input_quantized->allocation_type\
    \ = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims))\
    \ {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n\
    \      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n\
    \                                                       input_quantized_size));\n\
    \    }\n\n    // Tell interpreter to allocate temporary tensors to store scaling\
    \ factors.\n    node->temporaries->data[2] = scratch_tensor_index + 2;\n    TfLiteTensor*\
    \ scaling_factors;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node,\
    \ /*index=*/2,\n                                                &scaling_factors));\n\
    \    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type\
    \ = kTfLiteArenaRw;\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims,\
    \ 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n\
    \      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scaling_factors,\n                          \
    \                             scaling_factors_size));\n    }\n\n    // Used to\
    \ store dequantized weights_time matrix for hybrid computation of\n    // matmul(state,\
    \ weights_time), which occurs in floating point.\n    node->temporaries->data[3]\
    \ = scratch_tensor_index + 3;\n    TfLiteTensor* float_weights_time;\n    TF_LITE_ENSURE_OK(context,\
    \ GetTemporarySafe(context, node, /*index=*/3,\n                             \
    \                   &float_weights_time));\n    float_weights_time->type = kTfLiteFloat32;\n\
    \    // Persistent so that we can compute the dequantized weights only once.\n\
    \    float_weights_time->allocation_type = kTfLiteArenaRwPersistent;\n    if (!TfLiteIntArrayEqual(float_weights_time->dims,\
    \ weights_time->dims)) {\n      TfLiteIntArray* float_weights_time_size =\n  \
    \        TfLiteIntArrayCopy(weights_time->dims);\n      TF_LITE_ENSURE_OK(context,\n\
    \                        context->ResizeTensor(context, float_weights_time,\n\
    \                                              float_weights_time_size));\n  \
    \  }\n\n    node->temporaries->data[4] = scratch_tensor_index + 4;\n    TfLiteTensor*\
    \ zero_points;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context,\
    \ node, /*index=*/4, &zero_points));\n    zero_points->type = kTfLiteFloat32;\n\
    \    zero_points->allocation_type = kTfLiteArenaRw;\n    int zero_points_dims[1]\
    \ = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(zero_points->dims, 1, zero_points_dims))\
    \ {\n      TfLiteIntArray* zero_points_size = TfLiteIntArrayCreate(1);\n     \
    \ zero_points_size->data[0] = zero_points_dims[0];\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, zero_points,\n                              \
    \                         zero_points_size));\n    }\n\n    node->temporaries->data[5]\
    \ = scratch_tensor_index + 5;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n\
    \                      GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n\
    \    row_sums->type = kTfLiteFloat32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n\
    \    int row_sums_dims[1] = {num_filters};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims,\
    \ 1, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n\
    \      row_sums_size->data[0] = row_sums_dims[0];\n      TF_LITE_ENSURE_OK(\n\
    \          context, context->ResizeTensor(context, row_sums, row_sums_size));\n\
    \    }\n  }\n  if (is_full_integer) {\n    // Allocated one extra tensor.\n  \
    \  TfLiteIntArray* output_temp_size_array = TfLiteIntArrayCreate(2);\n    output_temp_size_array->data[0]\
    \ = num_units;\n    output_temp_size_array->data[1] = batch_size;\n    node->temporaries->data[1]\
    \ = scratch_tensor_index + 1;\n    TfLiteTensor* output_temp;\n    TF_LITE_ENSURE_OK(\n\
    \        context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\
    \    output_temp->type = kTfLiteInt32;\n    output_temp->allocation_type = kTfLiteArenaRw;\n\
    \    TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_temp,\n\
    \                                                     output_temp_size_array));\n\
    \n    // Calculate effective scales.\n    TF_LITE_ENSURE(context, input->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* input_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_feature->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_feature->quantization.params);\n    TF_LITE_ENSURE(context, state->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* state_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_time->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_time->quantization.params);\n    TF_LITE_ENSURE(context, output->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        output->quantization.params);\n    const double effective_scale_1 = input_params->scale->data[0]\
    \ *\n                                     weights_feature_params->scale->data[0]\
    \ /\n                                     state_params->scale->data[0];\n    const\
    \ double effective_scale_2 = state_params->scale->data[0] *\n                \
    \                     weight_time_params->scale->data[0] /\n                 \
    \                    output_params->scale->data[0];\n    QuantizeMultiplier(effective_scale_1,\
    \ &op_data->effective_scale_1_a,\n                       &op_data->effective_scale_1_b);\n\
    \    QuantizeMultiplier(effective_scale_2, &op_data->effective_scale_2_a,\n  \
    \                     &op_data->effective_scale_2_b);\n  }\n  return kTfLiteOk;\n\
    }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: gpt-4o
    model: gpt-4o
    model_type: gpt-4o
    temperature: 0.0
  memory:
    memory_type: judge
  name: review_board
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver: []
  role_description: "You are the Review Board. After reviewing the Moderator\xE2\u20AC\
    \u2122s summary and <code> (if needed, the original arguments), \nproduce a JSON\
    \ array of verdicts for each vulnerability identified by the Security Researcher.\
    \ Each object in the array should include:\n- `vulnerability`: The same name as\
    \ given by the Security Researcher.\n- `decision`: One of 'valid', 'invalid',\
    \ or 'partially valid'.\n- `severity`: If valid or partially valid, assign a severity\
    \ ('low', 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`:\
    \ Suggest what should be done next (e.g., 'fix immediately', 'monitor', 'no action\
    \ needed').\n- `justification`: A brief explanation of why you reached this conclusion,\
    \ considering both the Security Researcher\xE2\u20AC\u2122s and Code Author\xE2\
    \u20AC\u2122s perspectives.\n\nYou need to analyze the code and evaluate the reasoning\
    \ provided by the Security Researcher, Code Author, and Moderator. Do not automatically\
    \ mark a decision as 'valid' just because the Code Author refutes it, nor mark\
    \ it as 'invalid' because the Security Researcher claims a vulnerability exists.\
    \ Instead, carefully assess whether their reasoning aligns with the actual security\
    \ implications and technical reality.\n\n<code>:\nTfLiteStatus Prepare(TfLiteContext*\
    \ context, TfLiteNode* node) {\n  const auto* params = reinterpret_cast<TfLiteSVDFParams*>(node->builtin_data);\n\
    \  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  int scratch_tensor_index\
    \ = op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs\
    \ we need.\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TF_LITE_ENSURE_EQ(context,\
    \ node->inputs->size, 5);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* weights_feature;\n\
    \  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kWeightsFeatureTensor,\n\
    \                                          &weights_feature));\n  const TfLiteTensor*\
    \ weights_time;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node,\
    \ kWeightsTimeTensor, &weights_time));\n\n  TF_LITE_ENSURE(context,\n        \
    \         input->type == kTfLiteFloat32 || input->type == kTfLiteInt8);\n\n  //\
    \ Check all the parameters of tensor match within themselves and match the\n \
    \ // input configuration.\n  const int rank = params->rank;\n  const int batch_size\
    \ = input->dims->data[0];\n  const int num_filters = weights_feature->dims->data[0];\n\
    \  TF_LITE_ENSURE(context, rank != 0);\n  TF_LITE_ENSURE_EQ(context, num_filters\
    \ % rank, 0);\n  const int num_units = num_filters / rank;\n  const int memory_size\
    \ = weights_time->dims->data[1];\n  TF_LITE_ENSURE_EQ(context, input->dims->data[1],\n\
    \                    weights_feature->dims->data[1]);\n  TF_LITE_ENSURE_EQ(context,\
    \ weights_time->dims->data[0], num_filters);\n\n  const TfLiteTensor* bias = GetOptionalInputTensor(context,\
    \ node, kBiasTensor);\n  if (bias) {\n    TF_LITE_ENSURE_EQ(context, bias->dims->data[0],\
    \ num_units);\n  }\n\n  const TfLiteTensor* state;\n  TF_LITE_ENSURE_OK(context,\
    \ GetInputSafe(context, node, kStateTensor, &state));\n  TfLiteTensor* output;\n\
    \  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node,\
    \ kOutputTensor, &output));\n\n  // Check the shape of input state tensors.\n\
    \  TF_LITE_ENSURE_EQ(context, NumDimensions(state), 2);\n  TF_LITE_ENSURE_EQ(context,\
    \ SizeOfDimension(state, 0), batch_size);\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(state,\
    \ 1),\n                    memory_size * num_filters);\n\n  // Resize output.\n\
    \  TfLiteIntArray* output_size_array = TfLiteIntArrayCreate(2);\n  output_size_array->data[0]\
    \ = batch_size;\n  output_size_array->data[1] = num_units;\n  TF_LITE_ENSURE_OK(context,\n\
    \                    context->ResizeTensor(context, output, output_size_array));\n\
    \n  // The weights are of consistent type, so it suffices to check one.\n  const\
    \ bool is_hybrid_op = IsHybridOp(input, weights_feature);\n  const bool is_full_integer\
    \ = input->type == kTfLiteInt8;\n\n  // Resize scratch.\n  TfLiteIntArrayFree(node->temporaries);\n\
    \  if (is_hybrid_op) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  }\
    \ else if (is_full_integer) {\n    node->temporaries = TfLiteIntArrayCreate(2);\n\
    \  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[0]\
    \ = scratch_tensor_index;\n\n  TfLiteIntArray* scratch_size_array = TfLiteIntArrayCreate(2);\n\
    \  scratch_size_array->data[0] = batch_size;\n  scratch_size_array->data[1] =\
    \ num_filters;\n\n  TfLiteTensor* scratch_tensor;\n  TF_LITE_ENSURE_OK(\n    \
    \  context, GetTemporarySafe(context, node, /*index=*/0, &scratch_tensor));\n\n\
    \  // The scratch buffer is of type int32 for full integer svdf and it's of type\n\
    \  // float32 for hybrid and float case.\n  if (is_full_integer) {\n    scratch_tensor->type\
    \ = kTfLiteInt32;\n  } else {\n    scratch_tensor->type = kTfLiteFloat32;\n  }\n\
    \  scratch_tensor->allocation_type = kTfLiteArenaRw;\n  TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scratch_tensor,\n                           \
    \                        scratch_size_array));\n\n  if (is_hybrid_op) {\n    op_data->compute_row_sums\
    \ = true;\n    // Tell interpreter to allocate temporary tensors to store quantized\
    \ values\n    // of input tensors.\n    node->temporaries->data[1] = scratch_tensor_index\
    \ + 1;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context,\
    \ node, /*index=*/1,\n                                                &input_quantized));\n\
    \    input_quantized->type = weights_feature->type;\n    input_quantized->allocation_type\
    \ = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims))\
    \ {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n\
    \      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n\
    \                                                       input_quantized_size));\n\
    \    }\n\n    // Tell interpreter to allocate temporary tensors to store scaling\
    \ factors.\n    node->temporaries->data[2] = scratch_tensor_index + 2;\n    TfLiteTensor*\
    \ scaling_factors;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node,\
    \ /*index=*/2,\n                                                &scaling_factors));\n\
    \    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type\
    \ = kTfLiteArenaRw;\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims,\
    \ 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n\
    \      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, scaling_factors,\n                          \
    \                             scaling_factors_size));\n    }\n\n    // Used to\
    \ store dequantized weights_time matrix for hybrid computation of\n    // matmul(state,\
    \ weights_time), which occurs in floating point.\n    node->temporaries->data[3]\
    \ = scratch_tensor_index + 3;\n    TfLiteTensor* float_weights_time;\n    TF_LITE_ENSURE_OK(context,\
    \ GetTemporarySafe(context, node, /*index=*/3,\n                             \
    \                   &float_weights_time));\n    float_weights_time->type = kTfLiteFloat32;\n\
    \    // Persistent so that we can compute the dequantized weights only once.\n\
    \    float_weights_time->allocation_type = kTfLiteArenaRwPersistent;\n    if (!TfLiteIntArrayEqual(float_weights_time->dims,\
    \ weights_time->dims)) {\n      TfLiteIntArray* float_weights_time_size =\n  \
    \        TfLiteIntArrayCopy(weights_time->dims);\n      TF_LITE_ENSURE_OK(context,\n\
    \                        context->ResizeTensor(context, float_weights_time,\n\
    \                                              float_weights_time_size));\n  \
    \  }\n\n    node->temporaries->data[4] = scratch_tensor_index + 4;\n    TfLiteTensor*\
    \ zero_points;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context,\
    \ node, /*index=*/4, &zero_points));\n    zero_points->type = kTfLiteFloat32;\n\
    \    zero_points->allocation_type = kTfLiteArenaRw;\n    int zero_points_dims[1]\
    \ = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(zero_points->dims, 1, zero_points_dims))\
    \ {\n      TfLiteIntArray* zero_points_size = TfLiteIntArrayCreate(1);\n     \
    \ zero_points_size->data[0] = zero_points_dims[0];\n      TF_LITE_ENSURE_OK(context,\
    \ context->ResizeTensor(context, zero_points,\n                              \
    \                         zero_points_size));\n    }\n\n    node->temporaries->data[5]\
    \ = scratch_tensor_index + 5;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n\
    \                      GetTemporarySafe(context, node, /*index=*/5, &row_sums));\n\
    \    row_sums->type = kTfLiteFloat32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n\
    \    int row_sums_dims[1] = {num_filters};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims,\
    \ 1, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n\
    \      row_sums_size->data[0] = row_sums_dims[0];\n      TF_LITE_ENSURE_OK(\n\
    \          context, context->ResizeTensor(context, row_sums, row_sums_size));\n\
    \    }\n  }\n  if (is_full_integer) {\n    // Allocated one extra tensor.\n  \
    \  TfLiteIntArray* output_temp_size_array = TfLiteIntArrayCreate(2);\n    output_temp_size_array->data[0]\
    \ = num_units;\n    output_temp_size_array->data[1] = batch_size;\n    node->temporaries->data[1]\
    \ = scratch_tensor_index + 1;\n    TfLiteTensor* output_temp;\n    TF_LITE_ENSURE_OK(\n\
    \        context, GetTemporarySafe(context, node, /*index=*/1, &output_temp));\n\
    \    output_temp->type = kTfLiteInt32;\n    output_temp->allocation_type = kTfLiteArenaRw;\n\
    \    TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_temp,\n\
    \                                                     output_temp_size_array));\n\
    \n    // Calculate effective scales.\n    TF_LITE_ENSURE(context, input->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* input_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_feature->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_feature->quantization.params);\n    TF_LITE_ENSURE(context, state->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* state_params =\n        reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\n\
    \    TF_LITE_ENSURE(context,\n                   weights_time->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        weights_time->quantization.params);\n    TF_LITE_ENSURE(context, output->quantization.type\
    \ != kTfLiteNoQuantization);\n    auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n\
    \        output->quantization.params);\n    const double effective_scale_1 = input_params->scale->data[0]\
    \ *\n                                     weights_feature_params->scale->data[0]\
    \ /\n                                     state_params->scale->data[0];\n    const\
    \ double effective_scale_2 = state_params->scale->data[0] *\n                \
    \                     weight_time_params->scale->data[0] /\n                 \
    \                    output_params->scale->data[0];\n    QuantizeMultiplier(effective_scale_1,\
    \ &op_data->effective_scale_1_a,\n                       &op_data->effective_scale_1_b);\n\
    \    QuantizeMultiplier(effective_scale_2, &op_data->effective_scale_2_a,\n  \
    \                     &op_data->effective_scale_2_b);\n  }\n  return kTfLiteOk;\n\
    }"
  verbose: true
environment:
  env_type: judge
  id_save: 255807
  max_turns: 4
  rule:
    describer:
      type: basic
    order:
      type: judge
    selector:
      type: basic
    updater:
      type: basic
    visibility:
      type: all
  target: 0
  task_name: code_vulnerability_review
  unit_tests: None
prompts:
  code_author_role_prompt: "You are the Code Author of <code>. The Security Researcher\
    \ has presented a JSON array of alleged vulnerabilities. \nYou must respond as\
    \ if you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\xE2\u20AC\u2122s entry.\n- `response_type`: 'refutation' if you believe\
    \ this concern is unfounded, or 'mitigation' if you acknowledge it and propose\
    \ a workable solution.\n- `reason`: A concise explanation of why the vulnerability\
    \ is refuted or how you propose to mitigate it."
  moderator_role_prompt: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\xE2\u20AC\u2122s identified\
    \ vulnerabilities and the Code Author\xE2\u20AC\u2122s responses, \nprovide a\
    \ single JSON object with two fields:\n- `researcher_summary`: A concise summary\
    \ of the vulnerabilities and reasoning presented by the Security Researcher.\n\
    - `author_summary`: A concise summary of the Code Author\xE2\u20AC\u2122s counterarguments\
    \ or mitigation strategies."
  prompt: 'You are working in a programming team to check whether a code have a potential
    vulnerability in it.


    ${role_description}


    ${chat_history}'
  review_board_role_prompt: "You are the Review Board. After reviewing the Moderator\xE2\
    \u20AC\u2122s summary and <code> (if needed, the original arguments), \nproduce\
    \ a JSON array of verdicts for each vulnerability identified by the Security Researcher.\
    \ Each object in the array should include:\n- `vulnerability`: The same name as\
    \ given by the Security Researcher.\n- `decision`: One of 'valid', 'invalid',\
    \ or 'partially valid'.\n- `severity`: If valid or partially valid, assign a severity\
    \ ('low', 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`:\
    \ Suggest what should be done next (e.g., 'fix immediately', 'monitor', 'no action\
    \ needed').\n- `justification`: A brief explanation of why you reached this conclusion,\
    \ considering both the Security Researcher\xE2\u20AC\u2122s and Code Author\xE2\
    \u20AC\u2122s perspectives.\n\nYou need to analyze the code and evaluate the reasoning\
    \ provided by the Security Researcher, Code Author, and Moderator. Do not automatically\
    \ mark a decision as 'valid' just because the Code Author refutes it, nor mark\
    \ it as 'invalid' because the Security Researcher claims a vulnerability exists.\
    \ Instead, carefully assess whether their reasoning aligns with the actual security\
    \ implications and technical reality."
  security_researcher_role_prompt: "You are the Security Researcher. Identify all\
    \ potential security vulnerabilities in the given <code> snippet. \nProvide your\
    \ output as a JSON array. Each element in the array represents one identified\
    \ vulnerability and should include:\n- `vulnerability`: A short name or description\
    \ of the vulnerability.\n- `reason`: A detailed explanation of why this is a vulnerability\
    \ and how it could be exploited.\n- `impact`: The potential consequences if this\
    \ vulnerability were exploited.\n\nNow please analyze the following code."
