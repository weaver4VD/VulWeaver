agents:
- agent_type: conversation
  llm:
    llm_type: deepseek-reasoner
    model: deepseek-reasoner
    model_type: deepseek-reasoner
    temperature: 0.0
  memory:
    memory_type: judge
  name: security_researcher
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - code_author
  - moderator
  - review_board
  role_description: "You are the Security Researcher. Identify all potential security\
    \ vulnerabilities in the given <code> snippet. \nProvide your output as a JSON\
    \ array. Each element in the array represents one identified vulnerability and\
    \ should include:\n- `vulnerability`: A short name or description of the vulnerability.\n\
    - `reason`: A detailed explanation of why this is a vulnerability and how it could\
    \ be exploited.\n- `impact`: The potential consequences if this vulnerability\
    \ were exploited.\n\nNow please analyze the following code.\n\n<code>:\n  void\
    \ Compute(tensorflow::OpKernelContext* context) override {\n    for (int ngram_width\
    \ : ngram_widths_) {\n      OP_REQUIRES(\n          context, ngram_width > 0,\n\
    \          errors::InvalidArgument(\"ngram_widths must contain positive values\"\
    ));\n    }\n\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context,\
    \ context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\
    \n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"\
    data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\
    \n    // Validate that the splits are valid indices into data, only if there are\n\
    \    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n\
    \    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size >\
    \ 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split\
    \ == 0,\n                  errors::InvalidArgument(\"First split value must be\
    \ 0, got \",\n                                          prev_split));\n      for\
    \ (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i)\
    \ >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n\
    \        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n\
    \                        \"Invalid split value \", splits_vec(i), \", must be\
    \ in [\",\n                        prev_split, \", \", input_data_size, \"]\"\
    ));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context,\
    \ prev_split == input_data_size,\n                  errors::InvalidArgument(\n\
    \                      \"Last split value must be data size. Expected \",\n  \
    \                    input_data_size, \", got \", prev_split));\n    }\n\n   \
    \ int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n\
    \    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(),\
    \ &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\
    \n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size()\
    \ == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n\
    \                     context->allocate_output(0, data->shape(), &empty));\n \
    \     for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i]\
    \ = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for\
    \ (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) -\
    \ splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width :\
    \ ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n\
    \      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams\
    \ = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n\
    \    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n\
    \        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}),\
    \ &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for\
    \ (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n\
    \      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width\
    \ : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams\
    \ = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start,\
    \ num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n\
    \      // If we're preserving short sequences, check to see if no sequence was\n\
    \      // generated by comparing the current output start idx to the original\n\
    \      // one (ngram_splits_data). If no ngrams were generated, then they will\n\
    \      // be equal (since we increment output_start_idx by num_ngrams every\n\
    \      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx\
    \ == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n\
    \        // One legitimate reason to not have any ngrams when preserve_short_\n\
    \        // is true is if the sequence itself is empty. In that case, move on.\n\
    \        if (data_length == 0) {\n          continue;\n        }\n        // We\
    \ don't have to worry about dynamic padding sizes here: if padding\n        //\
    \ was dynamic, every sequence would have had sufficient padding to\n        //\
    \ generate at least one ngram.\n\n        // If reached here, pad_width should\
    \ be > 0, pad_width_ = -1,\n        // which indicates max(ngram_widths) - 1 cannot\
    \ be used here since\n        // ngram_width is not known.\n        OP_REQUIRES(\n\
    \            context, pad_width_ >= 0,\n            errors::InvalidArgument(\"\
    Pad width should be >= 0 when \"\n                                    \"preserve_short_sequences\
    \ is True and \"\n                                    \"ngram_widths are not provided,\
    \ got \",\n                                    pad_width_));\n        int ngram_width\
    \ = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams,\
    \ ngram_width);\n      }\n    }\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: deepseek-reasoner
    model: deepseek-reasoner
    model_type: deepseek-reasoner
    temperature: 0.0
  memory:
    memory_type: judge
  name: code_author
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - security_researcher
  - moderator
  - review_board
  role_description: "You are the Code Author of <code>. The Security Researcher has\
    \ presented a JSON array of alleged vulnerabilities. \nYou must respond as if\
    \ you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\u2019s entry.\n- `response_type`: 'refutation' if you believe this\
    \ concern is unfounded, or 'mitigation' if you acknowledge it and propose a workable\
    \ solution.\n- `reason`: A concise explanation of why the vulnerability is refuted\
    \ or how you propose to mitigate it.\n\n<code>:\n  void Compute(tensorflow::OpKernelContext*\
    \ context) override {\n    for (int ngram_width : ngram_widths_) {\n      OP_REQUIRES(\n\
    \          context, ngram_width > 0,\n          errors::InvalidArgument(\"ngram_widths\
    \ must contain positive values\"));\n    }\n\n    const tensorflow::Tensor* data;\n\
    \    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto&\
    \ input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor*\
    \ splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n\
    \    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate\
    \ that the splits are valid indices into data, only if there are\n    // splits\
    \ specified.\n    const int input_data_size = data->flat<tstring>().size();\n\
    \    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size >\
    \ 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split\
    \ == 0,\n                  errors::InvalidArgument(\"First split value must be\
    \ 0, got \",\n                                          prev_split));\n      for\
    \ (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i)\
    \ >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n\
    \        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n\
    \                        \"Invalid split value \", splits_vec(i), \", must be\
    \ in [\",\n                        prev_split, \", \", input_data_size, \"]\"\
    ));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context,\
    \ prev_split == input_data_size,\n                  errors::InvalidArgument(\n\
    \                      \"Last split value must be data size. Expected \",\n  \
    \                    input_data_size, \", got \", prev_split));\n    }\n\n   \
    \ int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n\
    \    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(),\
    \ &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\
    \n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size()\
    \ == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n\
    \                     context->allocate_output(0, data->shape(), &empty));\n \
    \     for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i]\
    \ = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for\
    \ (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) -\
    \ splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width :\
    \ ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n\
    \      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams\
    \ = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n\
    \    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n\
    \        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}),\
    \ &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for\
    \ (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n\
    \      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width\
    \ : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams\
    \ = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start,\
    \ num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n\
    \      // If we're preserving short sequences, check to see if no sequence was\n\
    \      // generated by comparing the current output start idx to the original\n\
    \      // one (ngram_splits_data). If no ngrams were generated, then they will\n\
    \      // be equal (since we increment output_start_idx by num_ngrams every\n\
    \      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx\
    \ == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n\
    \        // One legitimate reason to not have any ngrams when preserve_short_\n\
    \        // is true is if the sequence itself is empty. In that case, move on.\n\
    \        if (data_length == 0) {\n          continue;\n        }\n        // We\
    \ don't have to worry about dynamic padding sizes here: if padding\n        //\
    \ was dynamic, every sequence would have had sufficient padding to\n        //\
    \ generate at least one ngram.\n\n        // If reached here, pad_width should\
    \ be > 0, pad_width_ = -1,\n        // which indicates max(ngram_widths) - 1 cannot\
    \ be used here since\n        // ngram_width is not known.\n        OP_REQUIRES(\n\
    \            context, pad_width_ >= 0,\n            errors::InvalidArgument(\"\
    Pad width should be >= 0 when \"\n                                    \"preserve_short_sequences\
    \ is True and \"\n                                    \"ngram_widths are not provided,\
    \ got \",\n                                    pad_width_));\n        int ngram_width\
    \ = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams,\
    \ ngram_width);\n      }\n    }\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: deepseek-reasoner
    model: deepseek-reasoner
    model_type: deepseek-reasoner
    temperature: 0.0
  memory:
    memory_type: judge
  name: moderator
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver:
  - review_board
  - security_researcher
  - code_author
  role_description: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\u2019s identified vulnerabilities\
    \ and the Code Author\u2019s responses, \nprovide a single JSON object with two\
    \ fields:\n- `researcher_summary`: A concise summary of the vulnerabilities and\
    \ reasoning presented by the Security Researcher.\n- `author_summary`: A concise\
    \ summary of the Code Author\u2019s counterarguments or mitigation strategies.\n\
    \n<code>:\n  void Compute(tensorflow::OpKernelContext* context) override {\n \
    \   for (int ngram_width : ngram_widths_) {\n      OP_REQUIRES(\n          context,\
    \ ngram_width > 0,\n          errors::InvalidArgument(\"ngram_widths must contain\
    \ positive values\"));\n    }\n\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context,\
    \ context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\
    \n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"\
    data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\
    \n    // Validate that the splits are valid indices into data, only if there are\n\
    \    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n\
    \    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size >\
    \ 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split\
    \ == 0,\n                  errors::InvalidArgument(\"First split value must be\
    \ 0, got \",\n                                          prev_split));\n      for\
    \ (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i)\
    \ >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n\
    \        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n\
    \                        \"Invalid split value \", splits_vec(i), \", must be\
    \ in [\",\n                        prev_split, \", \", input_data_size, \"]\"\
    ));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context,\
    \ prev_split == input_data_size,\n                  errors::InvalidArgument(\n\
    \                      \"Last split value must be data size. Expected \",\n  \
    \                    input_data_size, \", got \", prev_split));\n    }\n\n   \
    \ int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n\
    \    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(),\
    \ &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\
    \n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size()\
    \ == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n\
    \                     context->allocate_output(0, data->shape(), &empty));\n \
    \     for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i]\
    \ = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for\
    \ (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) -\
    \ splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width :\
    \ ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n\
    \      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams\
    \ = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n\
    \    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n\
    \        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}),\
    \ &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for\
    \ (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n\
    \      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width\
    \ : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams\
    \ = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start,\
    \ num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n\
    \      // If we're preserving short sequences, check to see if no sequence was\n\
    \      // generated by comparing the current output start idx to the original\n\
    \      // one (ngram_splits_data). If no ngrams were generated, then they will\n\
    \      // be equal (since we increment output_start_idx by num_ngrams every\n\
    \      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx\
    \ == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n\
    \        // One legitimate reason to not have any ngrams when preserve_short_\n\
    \        // is true is if the sequence itself is empty. In that case, move on.\n\
    \        if (data_length == 0) {\n          continue;\n        }\n        // We\
    \ don't have to worry about dynamic padding sizes here: if padding\n        //\
    \ was dynamic, every sequence would have had sufficient padding to\n        //\
    \ generate at least one ngram.\n\n        // If reached here, pad_width should\
    \ be > 0, pad_width_ = -1,\n        // which indicates max(ngram_widths) - 1 cannot\
    \ be used here since\n        // ngram_width is not known.\n        OP_REQUIRES(\n\
    \            context, pad_width_ >= 0,\n            errors::InvalidArgument(\"\
    Pad width should be >= 0 when \"\n                                    \"preserve_short_sequences\
    \ is True and \"\n                                    \"ngram_widths are not provided,\
    \ got \",\n                                    pad_width_));\n        int ngram_width\
    \ = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams,\
    \ ngram_width);\n      }\n    }\n  }"
  verbose: true
- agent_type: conversation
  llm:
    llm_type: deepseek-reasoner
    model: deepseek-reasoner
    model_type: deepseek-reasoner
    temperature: 0.0
  memory:
    memory_type: judge
  name: review_board
  output_parser:
    type: vultrial
  prompt_template: 'You are working in a programming team to check whether a code
    have a potential vulnerability in it.


    ${role_description}


    ${chat_history}'
  receiver: []
  role_description: "You are the Review Board. After reviewing the Moderator\u2019\
    s summary and <code> (if needed, the original arguments), \nproduce a JSON array\
    \ of verdicts for each vulnerability identified by the Security Researcher. Each\
    \ object in the array should include:\n- `vulnerability`: The same name as given\
    \ by the Security Researcher.\n- `decision`: One of 'valid', 'invalid', or 'partially\
    \ valid'.\n- `severity`: If valid or partially valid, assign a severity ('low',\
    \ 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`: Suggest\
    \ what should be done next (e.g., 'fix immediately', 'monitor', 'no action needed').\n\
    - `justification`: A brief explanation of why you reached this conclusion, considering\
    \ both the Security Researcher\u2019s and Code Author\u2019s perspectives.\n\n\
    You need to analyze the code and evaluate the reasoning provided by the Security\
    \ Researcher, Code Author, and Moderator. Do not automatically mark a decision\
    \ as 'valid' just because the Code Author refutes it, nor mark it as 'invalid'\
    \ because the Security Researcher claims a vulnerability exists. Instead, carefully\
    \ assess whether their reasoning aligns with the actual security implications\
    \ and technical reality.\n\n<code>:\n  void Compute(tensorflow::OpKernelContext*\
    \ context) override {\n    for (int ngram_width : ngram_widths_) {\n      OP_REQUIRES(\n\
    \          context, ngram_width > 0,\n          errors::InvalidArgument(\"ngram_widths\
    \ must contain positive values\"));\n    }\n\n    const tensorflow::Tensor* data;\n\
    \    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto&\
    \ input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor*\
    \ splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n\
    \    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate\
    \ that the splits are valid indices into data, only if there are\n    // splits\
    \ specified.\n    const int input_data_size = data->flat<tstring>().size();\n\
    \    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size >\
    \ 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split\
    \ == 0,\n                  errors::InvalidArgument(\"First split value must be\
    \ 0, got \",\n                                          prev_split));\n      for\
    \ (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i)\
    \ >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n\
    \        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n\
    \                        \"Invalid split value \", splits_vec(i), \", must be\
    \ in [\",\n                        prev_split, \", \", input_data_size, \"]\"\
    ));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context,\
    \ prev_split == input_data_size,\n                  errors::InvalidArgument(\n\
    \                      \"Last split value must be data size. Expected \",\n  \
    \                    input_data_size, \", got \", prev_split));\n    }\n\n   \
    \ int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n\
    \    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(),\
    \ &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\
    \n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size()\
    \ == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n\
    \                     context->allocate_output(0, data->shape(), &empty));\n \
    \     for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i]\
    \ = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for\
    \ (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) -\
    \ splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width :\
    \ ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n\
    \      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams\
    \ = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n\
    \    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n\
    \        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}),\
    \ &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for\
    \ (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n\
    \      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width\
    \ : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams\
    \ = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start,\
    \ num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n\
    \      // If we're preserving short sequences, check to see if no sequence was\n\
    \      // generated by comparing the current output start idx to the original\n\
    \      // one (ngram_splits_data). If no ngrams were generated, then they will\n\
    \      // be equal (since we increment output_start_idx by num_ngrams every\n\
    \      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx\
    \ == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n\
    \        // One legitimate reason to not have any ngrams when preserve_short_\n\
    \        // is true is if the sequence itself is empty. In that case, move on.\n\
    \        if (data_length == 0) {\n          continue;\n        }\n        // We\
    \ don't have to worry about dynamic padding sizes here: if padding\n        //\
    \ was dynamic, every sequence would have had sufficient padding to\n        //\
    \ generate at least one ngram.\n\n        // If reached here, pad_width should\
    \ be > 0, pad_width_ = -1,\n        // which indicates max(ngram_widths) - 1 cannot\
    \ be used here since\n        // ngram_width is not known.\n        OP_REQUIRES(\n\
    \            context, pad_width_ >= 0,\n            errors::InvalidArgument(\"\
    Pad width should be >= 0 when \"\n                                    \"preserve_short_sequences\
    \ is True and \"\n                                    \"ngram_widths are not provided,\
    \ got \",\n                                    pad_width_));\n        int ngram_width\
    \ = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n\
    \        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams,\
    \ ngram_width);\n      }\n    }\n  }"
  verbose: true
environment:
  env_type: judge
  id_save: 225125
  max_turns: 4
  rule:
    describer:
      type: basic
    order:
      type: judge
    selector:
      type: basic
    updater:
      type: basic
    visibility:
      type: all
  target: 0
  task_name: code_vulnerability_review
  unit_tests: None
prompts:
  code_author_role_prompt: "You are the Code Author of <code>. The Security Researcher\
    \ has presented a JSON array of alleged vulnerabilities. \nYou must respond as\
    \ if you are presenting your case to a group of decision-makers who will evaluate\
    \ each claim. \nYour tone should be respectful, authoritative, and confident,\
    \ as if you are defending the integrity of your work to a panel of experts.\n\n\
    For each identified vulnerability, produce a corresponding JSON object with the\
    \ following fields:\n- `vulnerability`: The same name/description from the Security\
    \ Researcher\u2019s entry.\n- `response_type`: 'refutation' if you believe this\
    \ concern is unfounded, or 'mitigation' if you acknowledge it and propose a workable\
    \ solution.\n- `reason`: A concise explanation of why the vulnerability is refuted\
    \ or how you propose to mitigate it."
  moderator_role_prompt: "You are the Moderator, and your role is to provide a neutral\
    \ summary. \nAfter reviewing both the Security Researcher\u2019s identified vulnerabilities\
    \ and the Code Author\u2019s responses, \nprovide a single JSON object with two\
    \ fields:\n- `researcher_summary`: A concise summary of the vulnerabilities and\
    \ reasoning presented by the Security Researcher.\n- `author_summary`: A concise\
    \ summary of the Code Author\u2019s counterarguments or mitigation strategies."
  prompt: 'You are working in a programming team to check whether a code have a potential
    vulnerability in it.


    ${role_description}


    ${chat_history}'
  review_board_role_prompt: "You are the Review Board. After reviewing the Moderator\u2019\
    s summary and <code> (if needed, the original arguments), \nproduce a JSON array\
    \ of verdicts for each vulnerability identified by the Security Researcher. Each\
    \ object in the array should include:\n- `vulnerability`: The same name as given\
    \ by the Security Researcher.\n- `decision`: One of 'valid', 'invalid', or 'partially\
    \ valid'.\n- `severity`: If valid or partially valid, assign a severity ('low',\
    \ 'medium', 'high'); if invalid, use 'none'.\n- `recommended_action`: Suggest\
    \ what should be done next (e.g., 'fix immediately', 'monitor', 'no action needed').\n\
    - `justification`: A brief explanation of why you reached this conclusion, considering\
    \ both the Security Researcher\u2019s and Code Author\u2019s perspectives.\n\n\
    You need to analyze the code and evaluate the reasoning provided by the Security\
    \ Researcher, Code Author, and Moderator. Do not automatically mark a decision\
    \ as 'valid' just because the Code Author refutes it, nor mark it as 'invalid'\
    \ because the Security Researcher claims a vulnerability exists. Instead, carefully\
    \ assess whether their reasoning aligns with the actual security implications\
    \ and technical reality."
  security_researcher_role_prompt: "You are the Security Researcher. Identify all\
    \ potential security vulnerabilities in the given <code> snippet. \nProvide your\
    \ output as a JSON array. Each element in the array represents one identified\
    \ vulnerability and should include:\n- `vulnerability`: A short name or description\
    \ of the vulnerability.\n- `reason`: A detailed explanation of why this is a vulnerability\
    \ and how it could be exploited.\n- `impact`: The potential consequences if this\
    \ vulnerability were exploited.\n\nNow please analyze the following code."
