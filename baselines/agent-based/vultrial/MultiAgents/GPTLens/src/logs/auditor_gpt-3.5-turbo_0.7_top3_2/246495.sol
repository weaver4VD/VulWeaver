[
    {
        "function_name": "Eval",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Potential Division by Zero",
        "reason": "The code does not handle the case where division by zero may occur. If the divisor (input2) is zero, it will lead to a division by zero error which can crash the program or lead to unexpected behavior. Attackers can exploit this vulnerability to potentially cause a denial of service or manipulate the program's execution flow.",
        "file_name": "246495.sol"
    },
    {
        "function_name": "TF_LITE_CHECK_DIV_NON_ZERO",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Lack of Boundary Checking",
        "reason": "The function TF_LITE_CHECK_DIV_NON_ZERO does not perform boundary checking to ensure that the index 'i' does not exceed the bounds of the input2_data array. If an attacker can control the value of 'i' and set it to a value greater than input2_elements, it can lead to out-of-bounds access and potentially overwrite critical data in memory, resulting in a buffer overflow vulnerability.",
        "file_name": "246495.sol"
    },
    {
        "function_name": "TF_LITE_CHECK_DIV_NON_ZERO",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Potential Division by Zero",
        "reason": "The TF_LITE_CHECK_DIV_NON_ZERO function only checks if the divisor (input2) is not equal to zero for each element in the input tensor. However, it does not handle the case where the entire input2 tensor is zero. If all elements in input2 are zero, it will still result in a division by zero error when performing the division operation, which can be exploited by attackers to cause a denial of service or crash the program.",
        "file_name": "246495.sol"
    },
    {
        "function_name": "Eval",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Zero Division Vulnerability",
        "reason": "The code snippet iterates through elements of input2 tensor and checks if any element is zero before performing the division operation. However, this check is insufficient as it only ensures that the divisor is not zero. An attacker can still manipulate the input data to contain zeros leading to potential division by zero errors, causing denial of service or potential information leakage.",
        "file_name": "246495.sol"
    },
    {
        "function_name": "Eval",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Information Disclosure Vulnerability",
        "reason": "The code snippet reports an error message when the output tensor type is not one of the supported types for division. This error message can potentially disclose sensitive information about the system to an attacker, aiding them in crafting targeted attacks. It is important to avoid leaking such details to potential adversaries for security reasons.",
        "file_name": "246495.sol"
    },
    {
        "function_name": "Eval",
        "code": "TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n  if (output->type == kTfLiteFloat32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n  return kTfLiteOk;\n}",
        "vulnerability": "Error Handling Vulnerability",
        "reason": "The code snippet returns a generic error code 'kTfLiteError' when an unsupported output tensor type is encountered. This simplistic error handling mechanism does not provide detailed information about the root cause of the error, making it harder for developers to diagnose and fix issues. Attackers can leverage this lack of specific error messages to exploit vulnerabilities more effectively.",
        "file_name": "246495.sol"
    }
]