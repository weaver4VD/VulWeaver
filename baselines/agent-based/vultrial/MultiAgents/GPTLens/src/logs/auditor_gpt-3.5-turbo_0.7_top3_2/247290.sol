[
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Potential buffer overflow",
        "reason": "The code does not perform proper bounds checking before assigning the value of 'out_min_max' to 'clip_value_max_', which could lead to a buffer overflow if 'clip_value_max_' is smaller than 'out_min_max'. This could potentially allow an attacker to overwrite adjacent memory locations and execute arbitrary code.",
        "file_name": "247290.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Potential information disclosure",
        "reason": "The code assigns either 0.0f or -out_min_max to 'output_min->flat<float>()(0)' based on the value of 'is_non_negative'. If 'is_non_negative' is false, it could potentially leak sensitive information by exposing the negative value of 'out_min_max' in the output.",
        "file_name": "247290.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Potential race condition",
        "reason": "The code uses OpenMP parallelization with reduction on 'out_min_max'. If multiple threads concurrently update 'out_min_max' without proper synchronization mechanisms, it can lead to a race condition where the final value of 'out_min_max' is non-deterministic and incorrect. This could be exploited by an attacker to manipulate the output results.",
        "file_name": "247290.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Sensitive Data Exposure",
        "reason": "The code directly accesses and stores sensitive data (input_min and input_max) without proper encryption or protection. This could lead to potential data leakage if an attacker gains unauthorized access to the memory.",
        "file_name": "247290.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Race Condition",
        "reason": "The parallel for loop is using OpenMP directives for parallel execution. If not handled properly, this can lead to race conditions where multiple threads access and manipulate shared data concurrently, potentially causing unexpected behavior and data corruption.",
        "file_name": "247290.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "vulnerability": "Integer Overflow",
        "reason": "The code performs arithmetic operations without proper bounds checking, potentially leading to integer overflow vulnerabilities. This could allow an attacker to manipulate input values to trigger unexpected behavior and potentially exploit the system.",
        "file_name": "247290.sol"
    }
]