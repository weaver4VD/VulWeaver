[
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Potential Denial of Service (DoS) vulnerability",
        "reason": "The code does not handle exceptions properly and does not have proper error handling mechanisms in place. An attacker could potentially trigger exceptions in a controlled manner to cause the application to crash, leading to a Denial of Service (DoS) attack.",
        "file_name": "197239.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Information Disclosure vulnerability",
        "reason": "The code does not implement proper input validation or sanitization techniques. This could lead to potential information disclosure if an attacker provides malicious input data to the program, causing it to leak sensitive information or behave unexpectedly.",
        "file_name": "197239.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Potential Remote Code Execution (RCE) vulnerability",
        "reason": "The code uses const_cast to cast away the const qualifier from pointers, which can lead to undefined behavior and potential memory corruption. An attacker could exploit this vulnerability to execute arbitrary code in the context of the application, leading to a Remote Code Execution (RCE) attack.",
        "file_name": "197239.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Potential Memory Corruption Vulnerability",
        "reason": "The code uses const_cast to convert const void pointer to a mutable float pointer. Since the original pointer was declared as const, attempting to modify it through the converted mutable pointer can lead to undefined behavior, potentially causing memory corruption.",
        "file_name": "197239.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Exception Handling Vulnerability",
        "reason": "The catch block in the code catches exceptions of type mkldnn::error but does not handle them securely. The error message is directly concatenated with other strings to form an error message without proper sanitization, which can potentially lead to information disclosure or other security issues.",
        "file_name": "197239.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }",
        "vulnerability": "Insecure Memory Allocation",
        "reason": "The code dynamically allocates memory using std::unique_ptr without proper error handling or exception safety measures. In case of memory allocation failure or exceptions during memory allocation, the code does not handle these scenarios gracefully, potentially leading to resource leaks or crashes.",
        "file_name": "197239.sol"
    }
]