[
    {
        "function_name": "Compute",
        "vulnerability": "Potential race condition",
        "criticism": "The reasoning for potential race condition is valid. The code uses OpenMP parallelization with reduction on 'out_min_max', which can lead to a race condition if not handled properly. The correctness, severity, and profitability scores are high as race conditions can cause unpredictable behavior and data corruption.",
        "correctness": 9,
        "severity": 9,
        "profitability": 9,
        "reason": "The code uses OpenMP parallelization with reduction on 'out_min_max'. If multiple threads concurrently update 'out_min_max' without proper synchronization mechanisms, it can lead to a race condition where the final value of 'out_min_max' is non-deterministic and incorrect. This could be exploited by an attacker to manipulate the output results.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 9.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Race Condition",
        "criticism": "The reasoning for race condition is valid. The parallel for loop using OpenMP directives can lead to race conditions if not handled properly, causing unexpected behavior and data corruption. The correctness, severity, and profitability scores are high due to the potential impact of race conditions.",
        "correctness": 9,
        "severity": 9,
        "profitability": 9,
        "reason": "The parallel for loop is using OpenMP directives for parallel execution. If not handled properly, this can lead to race conditions where multiple threads access and manipulate shared data concurrently, potentially causing unexpected behavior and data corruption.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 9.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Sensitive Data Exposure",
        "criticism": "The reasoning for sensitive data exposure is valid. The code directly accesses and stores sensitive data without encryption or protection, which could lead to data leakage. The correctness, severity, and profitability scores are moderate as the vulnerability could result in data exposure.",
        "correctness": 6,
        "severity": 6,
        "profitability": 6,
        "reason": "The code directly accesses and stores sensitive data (input_min and input_max) without proper encryption or protection. This could lead to potential data leakage if an attacker gains unauthorized access to the memory.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 6.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Potential buffer overflow",
        "criticism": "The reasoning for potential buffer overflow is incorrect. The code does perform proper bounds checking before assigning the value of 'out_min_max' to 'clip_value_max_'. Therefore, the vulnerability is not valid. The correctness score is high due to the incorrect reasoning. The severity and profitability scores are low as there is no actual vulnerability present.",
        "correctness": 8,
        "severity": 2,
        "profitability": 2,
        "reason": "The code does not perform proper bounds checking before assigning the value of 'out_min_max' to 'clip_value_max_', which could lead to a buffer overflow if 'clip_value_max_' is smaller than 'out_min_max'. This could potentially allow an attacker to overwrite adjacent memory locations and execute arbitrary code.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 5.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Potential information disclosure",
        "criticism": "The reasoning for potential information disclosure is valid. The code does expose the negative value of 'out_min_max' in the output if 'is_non_negative' is false, which could leak sensitive information. The correctness, severity, and profitability scores are moderate as the vulnerability could lead to data exposure.",
        "correctness": 5,
        "severity": 5,
        "profitability": 5,
        "reason": "The code assigns either 0.0f or -out_min_max to 'output_min->flat<float>()(0)' based on the value of 'is_non_negative'. If 'is_non_negative' is false, it could potentially leak sensitive information by exposing the negative value of 'out_min_max' in the output.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 5.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Integer Overflow",
        "criticism": "The reasoning for integer overflow is incorrect. The code does not perform arithmetic operations that could lead to integer overflow vulnerabilities. Therefore, the vulnerability is not valid. The correctness score is high due to the incorrect reasoning. The severity and profitability scores are low as there is no actual vulnerability present.",
        "correctness": 8,
        "severity": 2,
        "profitability": 2,
        "reason": "The code performs arithmetic operations without proper bounds checking, potentially leading to integer overflow vulnerabilities. This could allow an attacker to manipulate input values to trigger unexpected behavior and potentially exploit the system.",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n    auto transposed_input = input_matrix.shuffle(shuffling);\n    float out_min_max = std::numeric_limits<float>::min();\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  \n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));\n      float scale =\n          std::max(std::abs(input_min_data[i]), std::abs(input_max_data[i]));\n      ranges[i] =\n          scale * static_cast<float>(abs_max) / static_cast<float>(1L << 31);\n      if (min_per_channel < 0) is_non_negative = false;\n      out_min_max = std::max(out_min_max, ranges[i]);\n    }\n    if (out_min_max > clip_value_max_) out_min_max = clip_value_max_;\n    Tensor* output_min = nullptr;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n    output_min->flat<float>()(0) = is_non_negative ? 0.0f : -out_min_max;\n    output_max->flat<float>()(0) = out_min_max;\n  }",
        "file_name": "247290.sol",
        "final_score": 5.0
    }
]