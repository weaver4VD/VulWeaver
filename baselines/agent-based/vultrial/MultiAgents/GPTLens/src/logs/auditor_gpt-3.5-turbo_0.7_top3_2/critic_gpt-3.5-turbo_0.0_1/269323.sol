[
    {
        "function_name": "CoordinatesToFlatIndex",
        "vulnerability": "Integer Overflow",
        "criticism": "The reasoning provided for the vulnerability is valid as the function does not perform bounds checking on the input coordinates and strides arrays, which could lead to integer overflow. However, the severity of this vulnerability is not very high as it would require specific manipulation of input data to exploit. The correctness score is lower due to the lack of explicit validation for potential overflow scenarios. The severity score is moderate as the impact of an overflow may not be critical. The profitability score is also low as exploiting this vulnerability would require intricate knowledge of the system.",
        "correctness": 4,
        "severity": 5,
        "profitability": 2,
        "reason": "The function CoordinatesToFlatIndex does not perform bounds checking on the input coordinates and strides arrays. If the input coordinates or strides arrays are manipulated by an attacker to contain large values, it may cause an integer overflow vulnerability leading to unexpected behavior or potential security exploits.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    },
    {
        "function_name": "CoordinatesToFlatIndex",
        "vulnerability": "Denial of Service (DoS)",
        "criticism": "The reasoning provided for the vulnerability is valid as the function does not validate the input coordinates and strides arrays for negative values or inappropriate sizes, potentially leading to a DoS attack. The severity of this vulnerability is moderate as it could impact system availability. The correctness score is lower due to the lack of input validation. The profitability score is also moderate as a DoS attack could disrupt system operations.",
        "correctness": 4,
        "severity": 6,
        "profitability": 5,
        "reason": "The function CoordinatesToFlatIndex does not validate the input coordinates and strides arrays for negative values or inappropriate sizes. An attacker could exploit this vulnerability by providing malicious input to the function, causing it to enter an infinite loop or consume excessive computational resources, resulting in a denial of service (DoS) attack.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    },
    {
        "function_name": "CoordinatesToFlatIndex",
        "vulnerability": "Input Validation",
        "criticism": "The reasoning provided for the vulnerability is valid as the function assumes equal sizes of input arrays without explicit validation, which could lead to unexpected behavior. The correctness score is lower due to the assumption made without validation. The severity score is moderate as the impact of mismatched array sizes may not be critical. The profitability score is low as exploiting this vulnerability would require specific manipulation of input data.",
        "correctness": 4,
        "severity": 4,
        "profitability": 2,
        "reason": "The function CoordinatesToFlatIndex assumes that the input coordinates and strides arrays are of equal size. However, there is no explicit validation of this assumption, which could lead to unexpected behavior if the arrays have different sizes. An attacker could potentially manipulate the arrays to exploit this vulnerability and cause the program to behave unexpectedly.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    },
    {
        "function_name": "CoordinatesToFlatIndex",
        "vulnerability": "Integer Overflow",
        "criticism": "The reasoning provided for the vulnerability is valid as the function does not perform validation for integer overflow scenarios, potentially leading to unexpected behavior or crashes. The correctness score is lower due to the lack of explicit checks for overflow. The severity score is moderate as an overflow may cause program instability. The profitability score is low as exploiting this vulnerability would require specific input manipulation.",
        "correctness": 4,
        "severity": 5,
        "profitability": 2,
        "reason": "The 'CoordinatesToFlatIndex' function does not perform any validation or checks for integer overflow when calculating the index. An attacker could provide large coordinates or strides that could result in an overflow, leading to unexpected behavior or potentially a crash.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    },
    {
        "function_name": "sp.Reorder",
        "vulnerability": "Unvalidated Input",
        "criticism": "The reasoning provided for the vulnerability is valid as the 'reorder_dims' array used in 'sp.Reorder' is not validated, potentially allowing an attacker to reorder the SparseTensor unexpectedly. The correctness score is lower due to the lack of validation for input data. The severity score is moderate as data corruption or leakage could occur. The profitability score is low as exploiting this vulnerability would require specific knowledge of the system.",
        "correctness": 4,
        "severity": 6,
        "profitability": 2,
        "reason": "The 'reduction.reorder_dims' used in the 'sp.Reorder' function is not validated for correctness or bounds. An attacker could manipulate the 'reorder_dims' array to reorder the SparseTensor in unexpected ways, potentially causing data corruption or leaking sensitive information.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    },
    {
        "function_name": "out_flat",
        "vulnerability": "Out-of-Bounds Write",
        "criticism": "The reasoning provided for the vulnerability is valid as the code directly accesses 'out_flat' without bounds checking, potentially leading to an out-of-bounds write vulnerability. The correctness score is lower due to the lack of bounds checking. The severity score is moderate as critical data could be overwritten. The profitability score is low as exploiting this vulnerability would require specific manipulation of 'idx'.",
        "correctness": 4,
        "severity": 6,
        "profitability": 2,
        "reason": "The code directly accesses 'out_flat' at the index 'idx' without performing any bounds checking. An attacker could provide a crafted 'idx' value that is outside the bounds of 'out_flat', leading to an out-of-bounds write vulnerability. This could potentially overwrite critical data or lead to a crash.",
        "code": "void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  \n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  \n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }",
        "file_name": "269323.sol"
    }
]