[
    {
        "function_name": "Compute",
        "vulnerability": "Path Traversal",
        "criticism": "The reasoning correctly identifies a potential path traversal vulnerability due to the lack of validation or sanitization of the `ckpt_path` input. However, the severity of this vulnerability depends on the context in which the function is used. If the environment restricts file access to a safe directory, the impact is minimized. The profitability of exploiting this vulnerability is moderate, as it could lead to unauthorized file access or data leakage. Overall, the reasoning is mostly correct, but the severity and profitability could vary based on the deployment context.",
        "correctness": 8,
        "severity": 6,
        "profitability": 5,
        "reason": "The code uses a file path from `ckpt_path` tensor without sanitization or validation, allowing potential path traversal attacks. An attacker could manipulate the file path to access unauthorized files, leading to information disclosure or file corruption.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    },
    {
        "function_name": "Compute",
        "vulnerability": "Unvalidated Input",
        "criticism": "The reasoning highlights a valid concern regarding the use of `old_tensor_name` without validation. This could indeed lead to data tampering or denial of service if an attacker can manipulate the tensor name. The severity is moderate, as it could disrupt the function's operation or lead to incorrect data processing. The profitability is also moderate, as it depends on the attacker's ability to influence the input. The reasoning is accurate, but the impact may vary based on the specific use case.",
        "correctness": 8,
        "severity": 5,
        "profitability": 5,
        "reason": "The `old_tensor_name` is taken directly from user input without validation. This might allow an attacker to manipulate the tensor name to access or overwrite arbitrary tensors, potentially leading to data tampering or denial of service.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    },
    {
        "function_name": "Compute",
        "vulnerability": "Improper Bounds Checking",
        "criticism": "The reasoning correctly identifies that the function relies on assumptions about the mappings, which could lead to out-of-bounds access if those assumptions are violated. However, the function does include some bounds checking, which mitigates the risk to some extent. The severity is moderate, as out-of-bounds access could lead to crashes or data corruption. The profitability is low, as exploiting this would require specific conditions to be met. The reasoning is mostly correct, but the existing checks reduce the likelihood of exploitation.",
        "correctness": 7,
        "severity": 5,
        "profitability": 3,
        "reason": "While there is some bounds checking, the logic relies heavily on assumptions about the mappings and does not comprehensively validate indices before accessing elements. This can lead to out-of-bounds access if the assumptions are violated, potentially causing crashes or data corruption.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    },
    {
        "function_name": "Compute",
        "vulnerability": "Path Traversal Vulnerability",
        "criticism": "This reasoning is similar to the first vulnerability, identifying a potential path traversal issue with `ckpt_path`. The criticism remains the same: the vulnerability is valid, but its impact depends on the environment's file access restrictions. The severity and profitability are moderate, as unauthorized file access could lead to data leakage. The reasoning is correct, but the context of deployment affects the overall risk.",
        "correctness": 8,
        "severity": 6,
        "profitability": 5,
        "reason": "The code retrieves a file path from an input tensor, `ckpt_path_t`, without validating or sanitizing it, and uses it to open a file with `BundleReader`. This can lead to a path traversal vulnerability where an attacker could supply a path that accesses unauthorized files, potentially leading to data leakage or unauthorized access.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    },
    {
        "function_name": "Compute",
        "vulnerability": "Arbitrary File Access",
        "criticism": "The reasoning correctly points out that `old_tensor_name` is used without validation, which could allow an attacker to access arbitrary entries within the checkpoint file. This could expose sensitive data or cause unexpected behavior. The severity is moderate, as it could lead to data exposure or function disruption. The profitability is also moderate, depending on the attacker's ability to manipulate the input. The reasoning is accurate, but the impact may vary based on the specific use case.",
        "correctness": 8,
        "severity": 5,
        "profitability": 5,
        "reason": "The tensor name, retrieved from an input tensor `old_tensor_name_t`, is used directly to look up data type and shape in `reader.LookupDtypeAndShape`. Without validation, an attacker could manipulate `old_tensor_name` to point to arbitrary entries within the checkpoint file, potentially exposing sensitive data or causing unexpected behavior.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    },
    {
        "function_name": "Compute",
        "vulnerability": "Out-of-Bounds Write",
        "criticism": "The reasoning identifies a potential out-of-bounds write due to reliance on external mapping inputs. However, the function includes checks to ensure indices are within bounds before assignment, which reduces the risk. The severity is moderate, as out-of-bounds writes could lead to memory corruption or crashes. The profitability is low, as exploiting this would require specific conditions. The reasoning is partially correct, but the existing checks mitigate the risk.",
        "correctness": 7,
        "severity": 5,
        "profitability": 3,
        "reason": "The assignment to `output_matrix(new_row, new_col)` assumes that `new_row` and `new_col` are within the bounds of `output_matrix`. However, the bounds check is done before the assignment and relies on external mapping inputs which could be manipulated by an attacker to cause out-of-bounds writes, potentially leading to memory corruption or crashes.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol"
    }
]