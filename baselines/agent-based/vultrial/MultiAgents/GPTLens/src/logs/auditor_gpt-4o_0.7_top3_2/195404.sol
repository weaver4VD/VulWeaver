[
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Out-of-bounds access",
        "reason": "The loops iterate over ranges defined by height_start, height_end, width_start, and width_end. If these are not correctly bounded within the dimensions of in_mat, they could lead to out-of-bounds memory access, potentially causing crashes or undefined behavior.",
        "file_name": "195404.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Improper bounds calculation",
        "reason": "The calculation of height_end and width_end may not be properly bounded if the input tensor dimensions are not as expected or if pooling_ratio_ values are not validated. This can lead to incorrect memory access and potential data corruption.",
        "file_name": "195404.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Predictable random number generation",
        "reason": "If the seeds (seed_ and seed2_) are predictable or not securely generated, the random sequences generated can also be predictable. This makes the algorithm susceptible to attacks that exploit predictable outputs, such as guessing the sequence of operations.",
        "file_name": "195404.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Potential Buffer Overflow",
        "reason": "The code calculates offsets using input dimensions without validating if the computed offsets stay within the bounds of the allocated output tensor. This can lead to buffer overflow if `height_end` or `width_end` exceeds their respective maximums, potentially allowing an attacker to overwrite memory.",
        "file_name": "195404.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Improper Handling of Eigen Maps",
        "reason": "The Eigen maps are created directly from the tensor data without checking if the dimensions and sizes match the assumptions. If `input_size` or `output_size` are manipulated (e.g., by an attacker providing crafted input data), it could lead to out-of-bounds memory access.",
        "file_name": "195404.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    constexpr int tensor_in_and_out_dims = 4;\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }",
        "vulnerability": "Weak Randomness Initialization",
        "reason": "The `GuardedPhiloxRandom` generator is initialized with seeds `seed_` and `seed2_` without ensuring that they are derived from a secure source of entropy. This could lead to predictable random sequences, which attackers might exploit if the pooling sequences are used in security-sensitive applications.",
        "file_name": "195404.sol"
    }
]