[
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Uninitialized vector usage",
        "reason": "The vector 'sorted_order' is initialized with the size determined by 'truth_st_shape.dims()', but this size is not validated. If 'truth_st_shape.dims()' returns a negative value, it could lead to undefined behavior or memory corruption, making it exploitable if an attacker can influence 'truth_st_shape'.",
        "file_name": "195965.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Integer overflow in index calculation",
        "reason": "The calculation of 'loc' using 'std::inner_product' does not check for integer overflow, which could cause the 'loc' variable to wrap around to a negative value or larger than 'output_elements'. This is exploitable if the input tensors can be controlled by an attacker, potentially leading to buffer overflow.",
        "file_name": "195965.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Potential Denial of Service",
        "reason": "The function 'gtl::LevenshteinDistance' could be computationally expensive, especially if 'truth_seq' and 'hypothesis_seq' are large, leading to performance degradation. If an attacker can control the size of these sequences, they could exploit this to cause a Denial of Service by exhausting computational resources.",
        "file_name": "195965.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Improper Handling of High Dimensions",
        "reason": "The code initializes a vector `sorted_order` with the size of `truth_st_shape.dims()`, but does not validate the dimensions of `truth_st_shape`. If this value is excessively large, it can lead to excessive memory allocation or even crash the program due to resource exhaustion. Attackers could exploit this by providing input that results in a very high dimension, potentially causing a denial of service.",
        "file_name": "195965.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Integer Overflow in `std::inner_product`",
        "reason": "The `std::inner_product` function calculates a location `loc` to write the output tensor value. If the values in `g_truth` or `output_strides` are large, this operation can result in an integer overflow, leading to the calculation of `loc` being incorrect and potentially allowing writing outside the allocated buffer. This is only partially mitigated by the subsequent `OP_REQUIRES` check, which might not catch all overflow cases, especially if overflow results in a valid smaller number.",
        "file_name": "195965.sol"
    },
    {
        "function_name": "Compute",
        "code": "void Compute(OpKernelContext* ctx) override {\n    const Tensor* hypothesis_indices;\n    const Tensor* hypothesis_values;\n    const Tensor* hypothesis_shape;\n    const Tensor* truth_indices;\n    const Tensor* truth_values;\n    const Tensor* truth_shape;\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_indices\", &hypothesis_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_values\", &hypothesis_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"hypothesis_shape\", &hypothesis_shape));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_indices\", &truth_indices));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_values\", &truth_values));\n    OP_REQUIRES_OK(ctx, ctx->input(\"truth_shape\", &truth_shape));\n    OP_REQUIRES_OK(\n        ctx, ValidateShapes(ctx, *hypothesis_indices, *hypothesis_values,\n                            *hypothesis_shape, *truth_indices, *truth_values,\n                            *truth_shape));\n    TensorShape hypothesis_st_shape;\n    OP_REQUIRES_OK(ctx,\n                   TensorShapeUtils::MakeShape(\n                       hypothesis_shape->vec<int64_t>().data(),\n                       hypothesis_shape->NumElements(), &hypothesis_st_shape));\n    TensorShape truth_st_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            truth_shape->vec<int64_t>().data(),\n                            truth_shape->NumElements(), &truth_st_shape));\n    std::vector<int64_t> sorted_order(truth_st_shape.dims());\n    std::iota(sorted_order.begin(), sorted_order.end(), 0);\n    sparse::SparseTensor hypothesis;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *hypothesis_indices, *hypothesis_values,\n                            hypothesis_st_shape, sorted_order, &hypothesis));\n    sparse::SparseTensor truth;\n    OP_REQUIRES_OK(ctx, sparse::SparseTensor::Create(\n                            *truth_indices, *truth_values, truth_st_shape,\n                            sorted_order, &truth));\n    std::vector<int64_t> group_dims(truth_st_shape.dims() - 1);\n    std::iota(group_dims.begin(), group_dims.end(), 0);\n    TensorShape output_shape;\n    for (int d = 0; d < static_cast<int>(group_dims.size()); ++d) {\n      output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                   truth_st_shape.dim_size(d)));\n    }\n    const auto output_elements = output_shape.num_elements();\n    OP_REQUIRES(\n        ctx, output_elements > 0,\n        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n                                \" which has 0 elements\"));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n    auto output_t = output->flat<float>();\n    output_t.setZero();\n    std::vector<int64_t> output_strides(output_shape.dims());\n    output_strides[output_shape.dims() - 1] = 1;\n    for (int d = output_shape.dims() - 2; d >= 0; --d) {\n      output_strides[d] = output_strides[d + 1] * output_shape.dim_size(d + 1);\n    }\n    auto hypothesis_grouper = hypothesis.group(group_dims);\n    auto truth_grouper = truth.group(group_dims);\n    auto hypothesis_iter = hypothesis_grouper.begin();\n    auto truth_iter = truth_grouper.begin();\n    auto cmp = std::equal_to<T>();\n    while (hypothesis_iter != hypothesis_grouper.end() &&\n           truth_iter != truth_grouper.end()) {\n      sparse::Group truth_i = *truth_iter;\n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto truth_seq = truth_i.values<T>();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      if (g_truth == g_hypothesis) {\n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) =\n            gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n        if (normalize_) output_t(loc) /= truth_seq.size();\n        ++hypothesis_iter;\n        ++truth_iter;\n      } else if (g_truth > g_hypothesis) {  \n        auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = hypothesis_seq.size();\n        if (normalize_ && output_t(loc) != 0.0f) {\n          output_t(loc) = std::numeric_limits<float>::infinity();\n        }\n        ++hypothesis_iter;\n      } else {  \n        auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                      output_strides.begin(), int64_t{0});\n        OP_REQUIRES(\n            ctx, loc < output_elements,\n            errors::Internal(\"Got an inner product \", loc,\n                             \" which would require in writing to outside of \"\n                             \"the buffer for the output tensor (max elements \",\n                             output_elements, \")\"));\n        output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n        ++truth_iter;\n      }\n    }\n    while (hypothesis_iter != hypothesis_grouper.end()) {  \n      sparse::Group hypothesis_j = *hypothesis_iter;\n      std::vector<int64_t> g_hypothesis = hypothesis_j.group();\n      auto hypothesis_seq = hypothesis_j.values<T>();\n      auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = hypothesis_seq.size();\n      if (normalize_ && output_t(loc) != 0.0f) {\n        output_t(loc) = std::numeric_limits<float>::infinity();\n      }\n      ++hypothesis_iter;\n    }\n    while (truth_iter != truth_grouper.end()) {  \n      sparse::Group truth_i = *truth_iter;\n      std::vector<int64_t> g_truth = truth_i.group();\n      auto truth_seq = truth_i.values<T>();\n      auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                    output_strides.begin(), int64_t{0});\n      OP_REQUIRES(\n          ctx, loc < output_elements,\n          errors::Internal(\"Got an inner product \", loc,\n                           \" which would require in writing to outside of the \"\n                           \"buffer for the output tensor (max elements \",\n                           output_elements, \")\"));\n      output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n      ++truth_iter;\n    }\n  }",
        "vulnerability": "Potential Floating-point Exploitation",
        "reason": "Assigning infinity to elements of the output tensor can lead to undefined behavior in subsequent operations that use these values. If the output tensor is further processed without proper checks for infinity values, it could lead to crashes or incorrect computations, which can be exploited by providing crafted inputs that trigger such scenarios.",
        "file_name": "195965.sol"
    }
]