[
    {
        "function_name": "Compute",
        "vulnerability": "Buffer Overflow",
        "criticism": "The reasoning for the buffer overflow vulnerability is valid as the code lacks proper boundary checks when copying values. This could lead to buffer overflow vulnerabilities and potential crashes. The severity of this vulnerability is high as it could lead to code execution. The correctness score is high as the vulnerability reasoning is accurate. The severity score is high due to the potential impact of buffer overflow. The profitability score is moderate as successful exploitation could lead to code execution.",
        "correctness": 8,
        "severity": 9,
        "profitability": 7,
        "reason": "The code does not perform proper boundary checks when copying values from initializing_values to the output_matrix. This can lead to buffer overflow vulnerabilities where an attacker can manipulate the input values to write beyond the allocated memory, potentially causing a crash or executing malicious code.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 8.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Potential SQL Injection",
        "criticism": "The reasoning for the potential SQL injection vulnerability is valid as the code concatenates strings without proper sanitization, opening the door to SQL injection attacks. The severity of this vulnerability is high as it could lead to unauthorized access to the database. The correctness score is high as the vulnerability reasoning is accurate. The severity score is high due to the potential impact of SQL injection attacks. The profitability score is moderate as successful exploitation could lead to unauthorized access.",
        "correctness": 8,
        "severity": 8,
        "profitability": 7,
        "reason": "The code constructs SQL queries by concatenating strings without proper sanitization, which can lead to SQL injection attacks. Attackers can manipulate the input data to inject malicious SQL commands and potentially gain unauthorized access to the database.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 7.75
    },
    {
        "function_name": "Compute",
        "vulnerability": "Information Leakage",
        "criticism": "The reasoning for the information leakage vulnerability is valid as the code logs sensitive information to the console. This could potentially expose critical details to attackers. The severity of this vulnerability is moderate as it could aid attackers in crafting targeted attacks. The correctness score is high as the vulnerability reasoning is accurate. The severity score is moderate due to the potential impact of the information leakage. The profitability score is moderate as attackers could leverage this information for targeted attacks.",
        "correctness": 8,
        "severity": 6,
        "profitability": 6,
        "reason": "The code logs sensitive information such as the 'ckpt_path' and 'old_tensor_name' to the console using the LOG(INFO) statement. This can lead to information leakage and expose critical details about the system to potential attackers, aiding them in crafting targeted attacks.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 7.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Information Disclosure",
        "criticism": "The reasoning for the information disclosure vulnerability is valid as the code logs sensitive information to the console. Attackers could exploit this to gather valuable information for further attacks. The severity of this vulnerability is moderate as it could aid attackers in reconnaissance. The correctness score is high as the vulnerability reasoning is accurate. The severity score is moderate due to the potential impact of information disclosure. The profitability score is moderate as attackers could leverage this information for targeted attacks.",
        "correctness": 8,
        "severity": 6,
        "profitability": 6,
        "reason": "The code logs sensitive information such as checkpoint paths and row indices using LOG(INFO) statements. Attackers can exploit this vulnerability by accessing the log files to gather valuable information that can aid them in further attacks or reconnaissance.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 7.0
    },
    {
        "function_name": "Compute",
        "vulnerability": "Excessive Logging",
        "criticism": "The reasoning for the excessive logging vulnerability is valid as the code generates a significant amount of log data for each slice loaded. This could impact system performance and resource consumption. The severity of this vulnerability is moderate as it could affect system performance. The correctness score is high as the vulnerability reasoning is accurate. The severity score is moderate due to the potential impact on system performance. The profitability score is low as the impact is limited to system performance.",
        "correctness": 8,
        "severity": 6,
        "profitability": 3,
        "reason": "The code excessively logs debug information for each slice being loaded, which can lead to a significant amount of log data being generated. This can impact the performance of the system, consume unnecessary resources, and potentially expose sensitive information present in the loaded slices to unauthorized parties.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 6.25
    },
    {
        "function_name": "Compute",
        "vulnerability": "Potential Integer Overflow",
        "criticism": "The reasoning for the potential integer overflow vulnerability is valid as the code does not handle extremely large input values in the 'row_remapping' vector, which could lead to an integer overflow. However, the severity of this vulnerability is relatively low as it would require specific manipulation of input values to trigger. The correctness score is high as the vulnerability reasoning is accurate. The severity score is moderate due to the limited impact of the vulnerability. The profitability score is low as exploiting this vulnerability would be challenging and unlikely to yield significant benefits.",
        "correctness": 8,
        "severity": 5,
        "profitability": 2,
        "reason": "There is a potential integer overflow vulnerability in the loop that calculates the minimum and maximum old row values. If the input values in the 'row_remapping' vector are manipulated to be extremely large, it could lead to an integer overflow, resulting in unexpected behavior and potentially a security vulnerability.",
        "code": "void Compute(OpKernelContext* context) override {\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n    if (!remap_cols) {\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }",
        "file_name": "198013.sol",
        "final_score": 5.75
    }
]