CWE_409_RULE = """
[CWE-409 Reference Guideline | Improper Handling of Highly Compressed Data (Data Amplification) | Non-binding, Evidence-first]

Overview
- CWE-409 occurs when the product does not handle, or incorrectly handles, highly compressed input that expands to a very large output (high compression ratio), causing data amplification.
- This can lead to denial of service via excessive CPU, memory, disk usage, or bandwidth, when small inputs trigger disproportionately large decompressed outputs (e.g., “decompression bombs”).
- In partial snippets, the presence of decompression is not proof of vulnerability. Use this guideline to form hypotheses and identify code evidence that would confirm or refute them.

What counts as “data amplification via compression” (static analysis framing)
- Highly compressed input:
  - A compressed payload whose decompressed size is much larger than the compressed size (e.g., nested compression, repetitive patterns).
- Improper handling:
  - Decompressing without limiting output size, recursion/nesting depth, time, or resource consumption; or trusting metadata that claims safe sizes.
- Security-relevant impact:
  - Resource exhaustion (CPU, memory, disk), degraded availability, or cascading failures (queue backlog, OOM, service restarts).

Where CWE-409 typically occurs (sensitive sinks)
- Decompression routines:
  - GZIP/ZIP/DEFLATE/LZ4/Zstd/Brotli, archive extractors, document parsers (e.g., office/PDF-like bundles), image decoders with compressed streams.
- Automatic decompression in frameworks:
  - HTTP request/response decompression, message broker clients, storage gateways that transparently decompress.
- File upload / import pipelines:
  - Accepting user-supplied archives/documents and extracting/processing them server-side.
- Nested container formats:
  - Zip within zip, compressed attachments, compressed JSON/XML blobs within containers.

Common failure modes (reasoning hints, not assumptions)
- No output size cap:
  - Decompressing to memory/disk without a maximum decompressed size limit.
- Trusting untrusted size metadata:
  - Relying on “declared uncompressed size” or header fields without enforcement.
- Unlimited nesting/recursion:
  - Handling archives that can contain more archives, or multiple layers of compression, without depth limits.
- Streaming decompression to unbounded sinks:
  - Writing decompressed output to buffers/files without tracking cumulative bytes.
- Missing time/CPU budgets:
  - Decompression performed on request thread without throttling/timeouts, allowing CPU exhaustion.

What “robust decompression handling” looks like (context-specific)
- Enforce maximum decompressed bytes:
  - Track decompressed output size and stop when exceeding a configured threshold.
- Enforce maximum nesting and entry counts:
  - Limit archive depth, number of entries, and total extracted size across entries.
- Prefer streaming with bounded readers:
  - Use bounded input/output streams that enforce byte limits during decompression.
- Validate before and during processing:
  - Check compressed size, declared size (if present), and runtime expansion ratio; abort on suspicious ratios.
- Apply resource/time budgets:
  - Limit CPU time per decompression, offload to controlled workers, and apply rate limits.

Common weak/insufficient defenses (signals of possible risk)
- Checking only compressed size:
  - Small compressed input can still decompress to huge output.
- Post-hoc checks:
  - Checking size only after full decompression is too late (resources already consumed).
- Single-entry checks only:
  - Limiting per-file size but not total extracted size across many entries.
- Assuming trusted source:
  - Treating “internal uploads” as safe without considering attacker reachability through supply chain or SSRF-like paths.

Evidence to look for in code (static snippet oriented)
- Strong evidence of mitigation:
  - Bounded decompression with explicit limits (max bytes output, max entries, max depth).
  - Streaming decompression that counts bytes and aborts early on limit exceedance.
  - Guardrails for archive extraction: total size cap, per-entry size cap, entry count cap, and path safety where relevant.
  - Explicit handling of “declared size mismatch” and suspicious expansion ratios.
- Evidence suggesting vulnerability:
  - Directly decompressing into memory (byte arrays/strings) without size checks.
  - Extracting archives to disk without tracking total extracted bytes or entry count.
  - Loops that read from decompressor until EOF with no byte limit.
  - Accepting compressed input from external sources (uploads/requests/messages) and decompressing inline without controls.

How to use this guideline under incomplete context
- Identify compressed inputs and trust boundary:
  - Is input user-controlled (upload/request/message/file)? Is decompression automatic or explicit?
- Identify decompression/extraction sinks:
  - Where does decompressed data go (memory buffer, file, DB, downstream service)? Are byte counts tracked?
- Generate a small number of targeted hypotheses:
  - H1: Decompression runs without an upper bound on decompressed output size, enabling amplification DoS.
  - H2: Archive extraction lacks limits on total extracted size / entry count / nesting depth.
  - H3: The code trusts declared sizes/metadata and does not enforce runtime limits during decompression.
  - H4: Limits are enforced elsewhere (gateway, upload validator, bounded stream wrapper) → Unknown without evidence.
- For each hypothesis, specify:
  (a) evidence in the snippet supporting it (e.g., unbounded read loop, `toByteArray()`),
  (b) evidence needed to confirm it (e.g., configuration of max sizes, wrapper stream behavior),
  (c) counter-evidence that would refute it (e.g., explicit max-bytes counters, bounded streams, depth limits).

Quick red flags in partial code (weak → strong)
- Strong: Unbounded decompression/extraction of externally-supplied data into memory/disk (read-until-EOF with no byte cap).
- Medium: Per-entry checks exist but no total extracted size cap or nesting depth cap (multi-entry amplification).
- Medium: Reliance on declared uncompressed sizes without runtime enforcement.
- Weak: Decompression limits may be applied by omitted framework layers or wrappers → treat as Unknown and reduce confidence.
"""
if __name__ == "__main__":
    print(CWE_409_RULE)